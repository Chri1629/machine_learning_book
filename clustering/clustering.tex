\restylefloat{table,figure}
\pagestyle{fancy}
\cfoot{\thepage}
\renewcommand{\footrulewidth}{0.25pt}


 
\section{Clustering}
\subsection{Introduzione}

L'analisi di cluster è usata per risolvere moltissimi problemi pratici. In particolare l'\textbf{analisi di cluster tratta due diversi scopi generali:}

\begin{itemize}
	\item \textbf{Comprensione}: Le classi, o gruppi di oggetti che condividono caretteristiche, giocano un ruolo importante nella comprensione del mondo. Questo succede in biologia, in informatica e in economia.
	\item \textbf{Utilità}: Capacità di risassumere determinate carateristiche di un oggetto con le caratteristiche del cluster a cui appartiene. L'obiettivo è \textit{trovare i prototitpi con le proprietà più rappresentative dei cluster}.
\end{itemize}

Possiamo dare ora una definizione un po' più completa di analisi di clustering.
\begin{defn}
	La \textbf{cluster analysis} raggruppa i dati basandosi sulle informazioni trovate nei dati che descrivono gli oggetti e le loro relazioni.
\end{defn}

Gli obiettivi sono ora semplici da ridefinire:
\begin{itemize}
	\item Gli oggetti all'interno di un gruppo devono essere simili gli uni con gli altri, allo stesso tempo diversi (o incorrelati) con gli oggetti di altri gruppi.
	\item La più grande \textit{similarità} entro un gruppo deve corrispondere ad una grande \textit{differenza} tra i gruppi. 
\end{itemize}

\underline{Problema}: Come faccio a stabilire quando  e quanto degli oggetti sono simili? Non vi è un metodo per capirlo, tendenzialmente si utilizza una soluzione intermedia rispetto alle altre. 

La definizione di cluster è \textbf{intrinsicamente imprecisa},una migliore definizione dipende infatti dalla natura dei dati e dai risultati desiderati. 

\subsubsection{Tipi di clustering}

Formare un insieme di cluster è chiamato in gergo tecnico \textit{clustering.} Ci sono diversi tipi di analisi di clustering.

\begin{itemize}
	\item Partitional vs Hierarchical
	\item Exclusive vs  Overlapping vs Fuzzy
	\item Complete vs Partial.
\end{itemize}

Diamo ora una rapida definizione di tutte:

\begin{defn}
	Un clustering si dice \textbf{partizionale} se vi è una divisione del dataset in insiemi non sovrapposti tali che un elemento appartiene ad un solo insieme.
\end{defn}

\begin{defn}
	Un clustering si dice \textbf{gerarchico} se ogni cluster può essere a sua volta suddiviso in sotto cluster, in questo caso il clustering è un insieme di cluster che sono organizzati come un albero.
\end{defn}

\begin{defn}
	Un clustering si dice \textbf{esclusivo} se ogni oggetto è assegnato ad un singolo cluster.
\end{defn}

\begin{defn}
	Un clustering si dice \textbf{sovrapponibile} se ogni oggetto può essere assegnato a più di un cluster.
\end{defn}

\begin{defn}
	Un clustering si dice \textbf{fuzzy} se ogni oggetto può essere assegnato a più di un cluster contemporaneamente con un valore che tiene conto del peso che ha l'oggetto rispetto all'appartenenza ad un singolo cluster, la somma dei pesi deve essere necessariamente 1.
\end{defn}
\begin{figure}[H]
	\centering
	\includegraphics[height=0.3 \linewidth]{clustering/pict/fuzzy.png}
	\caption{Custering con modello fuzzy}
\end{figure}
\begin{defn}
	Un clustering si dice \textbf{completo} se ogni oggetto è assegnato ad un cluster (non ci sono oggetti liberi).
\end{defn}

\begin{defn}
	Un clustering si dice \textbf{parziale} se esiste almeno un oggetto che non è assegnato a nessun cluster, si usa perché potrebbero esserci degli outlier ed inserirli all'interno di un cluster potrebbe peggiorare in modo significativo la rappresentazione di un cluster.
\end{defn}

\subsubsection{Differenti nozioni di cluster}
I cluster naturali sono cluster che si dicano esistano per davvero anche se questo  è molto difficile che ciò accada. Per visualizzare le differenze tra i tipi diversi di dati sfruttiamo dati come punti a due dimensioni.
\begin{itemize}
	\item \textbf{Well separated Cluster:} dato un cluster, ogni oggetto è più vicino ad ogni oggetto del cluster a cui appartiene piuttosto che a qualsiasi altro oggetto di ogni altro cluster. Un cluster così ben formato permette di avere separazioni molto nette, questo tipo di cosa succede però molto raramente in realtà.
	\item \textbf{Prototpe-based cluster} dato un cluster, ogni oggetto di quel cluster è più vicino al prototipo che definisce il cluster rispetto ad ogni prototipo di un altro cluster. Il prototipo \`e solitamente il \textit{centroide} del cluster. Il \textit{prototipo} di un cluster corrisponde all'individuo meglio rappresentato dal cluster (può essere anche fittizio). Cluster fatti in questo modo tendon ad essere globulari.
	\item \textbf{Density-based cluster} un cluster è una regione densa di oggetti che sono circostritti da una regione di bassa densità. Questi sono usati quando i cluster sono irregolari o intermittenti oppure quando c'è una grande presenza di rumore o di outlier.
	\item \textbf{Graph-based cluster} se i dati sono rappresentati da grafi, i nodi rappresentano  oggetti e i collegamenti connettono gli oggettti. Allora ogni cluster è una componente connessa. La connessione può anche essere pesata e scelta in base ad una certa soglia. 
	Questi cluster sono molto utilizzati in quanto c'è un sacco di ricerca già fatta.
\end{itemize}

\subsubsection{Componenti di un'analisi di clustering}

Per prima cosa  avviene la \textit{feature selection} che assicura la trattenuta degli attributi del dataset degni di significato. Successivamente avviene la fase di  \textit{feature extraction} che serve a produrre feature che potrebbero andare meglio per scoprire la struttura dei dati, questa pratica potrebbe tuttavia generare features di difficile comprensione.
Bisognerebbe usare come feature ideali quelle che permettono di distinguere i pattern degli elementi che appartengono ai diversi cluster, immuni al rumore e facili da interpretare.

Il secondo passo è quello di \textit{determinare la misura di prossimità e costruire la funzione di merito.} Una volta che abbiamo determinato una misura di prossimità il problema di clustering si traduce in un problema di ottimizzazione con una specifica funzione.

Bisogna ricordare sempre che diversi algoritmi di dati permettono di avere conclusioni anche totalmente diverse, questo è il motivo per cui in principio non bisogna prediligere alcun algoritmo ma confrontare i risultati ottenuti e trarne conclusioni.


\subsection{ Proximity}

La proximity è uno strumento fondamentale per valutare il funzionamento di un algoritmo di clustering. Questa nfluenza quindi in modo pesante la nostra soluzione di un nostro problema di clustering. Bisogna fare una scelta della misura con cui approssimiamo quanto sono simili gli elementi appartenenti allo stesso cluster.
\subsubsection{Introduzione}
\textit{La analisi dei cluster affonda le sue radici nel concetto di cosa sia simile e cosa sia dissimile,} quando cerchiamo di esprimere questo concetto  in termini formali diventa abbastanza difficile. Questo avviene perché la similitudine dipende fortemente dal contesto analizzato .

In generale la similarità è nulla se i due oggetti sono totalmente differenti sotto la caratteristica che stiamo valutando ed è uguale a 1 se sono completamente uguali, è comune però trovare misure di similitudine che hanno come valori:.  $[0,\inf]$. Useremo il termine \textbf{proximity} per indicare sia la similarità che la dissimilarità. 

\begin{figure}[H]
	\centering
	\includegraphics[height=0.45 \linewidth]{clustering/pict/simil_diss.png}
	\caption{vantaggi e svantaggi tra tecniche uni-variate e multi-variate}
\end{figure}

Non c'è ortogonalità tra la scelta della misura e l'esito che otterrò. Nasce per questo motivo l'esigenza di poter passare da una misura all'altra, in particolare per trasformare una misura di similarità dall'intervallo $[0,\inf]$ all'intervallo $[0,1]$ si opera la seguente trasformazione:

\[s' = \frac{s - \min{s}}{\max{s} - \min{s}} \qquad d' = \frac{d - \min{d}}{\max{d} - \min{d}} \] 

Ci sono diversi problemi che nascono quando cambiamo l'intervallo in cui si trova il valore. Per farlo devo usare una trasformazione \textit{non-lineare}. Posso usare però una cosa del genere:

\[ d' = \frac{d}{1+d}\]

Con questa trasformazione grandi valori della dissimilarità $d$ vengono compressi in valori vicini a 1. Il fatto di distorcere o meno le distanze dipende dal compito che voglio svolgere.

Ci sono diversi problemi che nascono quando trasformiamo una similarità in dissimilarità e viceversa. Per farlo devo usare una trasformazione \textit{non-lineare}. Posso usare però una cosa del genere:

\[ se \quad s,d = [0, 1]  \quad allora  \quad s =  1-d\]

In generale si può usare qualsiasi funzione monotona decrescente per trasformare la similarità in dissimilarità.

\begin{defn}
	Si definisce prossimità tra due record come la funzione di prossimità tra i corrispondenti attributi dei due record.	
\end{defn}

Consideriamo in prima analisi la misura di prossimità tra due record aventi un solo attributo ed estendiamo successivamente questa analisi a record con più di un attributo.
\begin{figure}[H]
	\centering
	\includegraphics[height=0.3 \linewidth]{clustering/pict/proximity_one.png}
	\caption{Tabella per misurare la prossimità di record con un solo attributo in funzione del tipo di attributo}
\end{figure}

Consideriamo due record riferiti al medesimo attributo nominale qualitativo, tutto ciò che possiamo dire è se i due record hanno  lo stesso valore o meno.
Per quanto riguarda gli attributi binari la dissimilarità esclude la similarità con valori 0 e 1. 

Se ho attributi categoriciordinali assegno un valore intero agli stessi in base alla scala utilizzata e calcolo la dissimilarità come rapporto tra la differenza dei due record e la scala totale di valori di utilizzo. Naturalmente va notato che sto utilizzando una scala linerare, questa è ovviamente un'assunzione molto forte che però bisogna tenere in conto in quanto qualsiasi scala di valori scelta è fatta basandosi su assunzioni.

Come notiamo dalla tabella è molto più facile definire la prossimità tra due attributi numerici, essa è infatti definita come \textit{la differenza in modulo tra i due valori.}
\subsubsection{Misure della distanza}

Quando ho degli attributi numerici posso definire altre misure di distanza nel seguente modo:
\begin{figure}[H]
	\centering
	\includegraphics[height=0.3 \linewidth]{clustering/pict/distanze_minkowski.png}
	\caption{Misure di distanza a partire dalla distanza di Minkowski}
\end{figure}
Ricordiamo che le proprietà che una funzione deve soddisfare per essere definita distanza sono le seguenti:
La misura deve essere
\begin{itemize}
	\item Non negatività: $d(x,y) \geq 0 \quad \forall x,y \quad d(x,y) = 0 \quad iif \quad x = y$
	\item Simmetria $d(x,y) = d(y,x) \quad \forall x,y$
	\item Disuguaglianza triangolare $d(x,z) \leq d(x,y) + d(y,z) \quad \forall x,y,z$
\end{itemize}

La similarità \textit{non rispetta la disuguaglianza triangolare} ma solitamente verifica le proprietà di simmetria e non negatività.

Forniamo ora qualche esempio di misura di prossimità:
\begin{defn}
	 Si definisce \textbf{Simple matching coefficient} la seguente espressione: \[ SMC(x,y) = \frac{\#maching\_attributes}{\#attributes} =  \frac{f_{11}+ f_{00}}{f_{11}+ f_{00} + f_{01}+ f_{10}} \]
\end{defn}
Questa è una misura che ha senso per attributi \textit{simmetrici binari}, ossia per attributi che possono assumere solo i valori 0 o 1 in circa egual quantità.
 Questa misura è invece scomoda se non possiamo affermare se gli 0 siano veramente degli 0, per il valore 1 invece è chiaro. In questo caso si utilizza un'altra misura che è derivata da questa misura, ossia si usa l'assunzione per cui l'osservazione di un evento (identificata con 1), abbia peso maggiore della non osservazione di un evento (rappresentata con 0). Si definisce quindi il \textbf{coefficiente di Jaccard}

\begin{defn}
	Si definisce \textbf{Jaccard Coefficient} la seguente espressione:    \[ J(x,y) = \frac{\#maching\_attributes}{\#attributes\_except00} = \frac{f_{11}}{f_{11}+ f_{01}+ f_{10}}\]
\end{defn}


Questa è ovviamente una misura distorta rispetto agli 1 (infatti è un tipo di misura che va bene con attributi \textit{asimmetrici}) che mi permette però di focalizzare la mia attenzione sulla presenza di questi ultimi. 
Definiamo quindi un nuovo indice:

\begin{defn}
	Si definisce \textbf{Extended Jaccard Coefficient} la seguente espressione: \[ EJ(x,y) = \frac{x \cdot y}{||x||^2 + ||y||^2 - x \cdot y}\]
\end{defn}

 Questa misura è distorta per trattare dati sparsi, quindi tanti elementi in cui ho 0 e solo poche diverse da 0, questo si usa ad esempio nell'analisi del linguaggio naturale. Penso ad esempio ai tweet, una parola che c'è in una frase ha una rilevanza maggiore rispetto ad una parola non presente nella stessa frase.

\subsubsection{Altre misure di prossimità}
Esplicitiamo  ulteriori misure di prossimità.
\begin{defn}
	Si definisce \textbf{cosine similarity.} la seguente espressione:   \[ cos(x,y) = \frac{x \cdot y }{||x|| \cdot ||y||  } \]
\end{defn}

 Viene usata quando tutti gli attributi sono di natura numerica, e si ignorano i match di natura 00. Il vantaggio di questa misura rispetto a quella di Jaccard è che in grado di trattare anche attributi non binari. E' molto utile quindi per comparare record sparsi ed è molto usata in \textit{Information Retrieval} dove i documenti (rappresentati come conteggi di vettori) devono essere comparati.


\begin{defn}
	Si definisce \textbf{Correlazione} la seguente espressione:    \[ corr(x,y) = \frac{cov(x,y)}{std(x)std(y)}\]
\end{defn}
Questa è la stessa correlazione di Pearson ma non legata alle variabili aleatorie.

Elenchiamo ora diverse problematiche legate alle misure di prossimità:
\begin{itemize}
	\item Come si trattano attibuti su scale di ampiezza diverse e/o correlati?
	
	Per risolvere il primo problema faccio la seguente cosa: normalizzo i valori, se ciò non venisse fatto le distanze Euclidee tra i due valori risulterebbero totalmente distorte a favore del valore maggiore. 
	
	Quando gli attributi sono fortemente correlati invece il trucco sta nel fatto che la misura di similarità è molto simile al grado di correlazione tra questi attributi, in tal caso utilizziamo la distanza di Mahalnobis:
	
	\[Mahal(x,y) = (x- y)\Sigma^{-1}(x- y)^{T}\]
	
	Ovviamente questa è una distanza che va usata solo se tutti gli attributi sono numerici.
	\item Come si calcola la prossimità tra record composti da attributi di tipo diverso?
	
	 Per risolvere questo problema mi occupo di valutare tutte le misure di prossimità enunciate in precedenza stando coerenti col tipo di attributo trattato.  
	Dopo averlo fatto uso una varibile indicatrice $\delta_{k}$ per ogni attributo k come segue:
	
	$\delta_{k}$ vale:
	\begin{itemize}
		\item 0  se il k-esimo attributo è asimmetrico ed entrambi i valori hanno \\  valore 0,  o almeno uno dei record presenta un missing value
		\item 1 altrimenti.
	\end{itemize} 


	Una volta definite queste allora la similarità si calcola come:
	\[similarity(x,y) = \frac{\sum_{k=1}^{n}\delta_{k}s_{k}(x,y)}{\sum_{k=1}^{n}\delta_{k}}\]
	\item Come si tratta la prossimità quando gli attributi hanno diversa rilevanza, ossia quando gli attributi contribuiscono secondo pesi diversi all'analisi? 
	
	Per risolvere quest'ultimo problema procedo esattamente nel modo precedente assegnando però dei pesi, le formule risolutive diventano quindi:
	\[similarity(x,y) = \frac{\sum_{k=1}^{n}w_{k}\delta_{k}s_{k}(x,y)}{\sum_{k=1}^{n}\delta_{k}}\]
\end{itemize}

Come è facilmente intuibile risulta molto complesso sostenere tutta questa specificità. Per farlo cerchiamo di ricondurci alla seguente scelta:
\begin{itemize}
	\item Dati densi e continui: distanze metriche  come quella euclidea sono buone rappresentazioni.
	\item Dati sparsi, binari asimmetrici: misure della distanza che ignorano i match 00 come cosine, Jaccard e Extended Jaccard.
\end{itemize}

\subsection{Clustering Algorithms}
Passiamo ora a parlare degli algoritmi di Clustering veri e propri:
\subsubsection{Prototype Based}

L'approccio ai  \textit{Prototype-Based Clustering} si basa sull'assunzione che ogni cluster possa essere ben rappresentato da un unico punto chiamato \textbf{prototipo.} Ogni oggetto è quindi collocato nel cluster del prototipo a cui è più vicino.

\begin{figure}[H]
	\centering
	\includegraphics[height=0.4 \linewidth]{clustering/pict/prototype_cluster.png}
	\caption{Esempio di prototype-based clustering}
\end{figure}
Esistono diversi tipi di algoritmi basati sul prototipo a seconda delle seguenti caratteristiche:

\begin{itemize}
	\item Ogni oggetto deve appartenere ad un singolo cluster.
	\item Ogni record è nella condizione di appartenere a più di un cluster contemporaneamente.
	\item Il concetto di cluster è modellizzato con una distribuzione di tipo probabilistico.
	\item I cluster sono costretti ad avere relazioni fissate.
\end{itemize}

\paragraph{K-medie}

Partiamo ora della descrizione di uno dei primi algoritmi basati sul prototipo. In questo caso il prototipo prende il nome di \textit{centroide,} questo valore solitamente è identificato dal vettore media dei valori degli attributi delle osservazioni di quel determinato cluster. Non siamo vincolati a ragionare in due dimensioni, il cluster generalmente è applicato ad oggetti in uno spazio continuo n-dimensionale.

Forniamo ora una descrizione schematica dell'algoritmo:
\begin{itemize}
	\item Scegliere per $k = 0$ quali sono i centroidi facendo in modo che non si "pestino i piedi".
	\item Ripetere queste cose:
	\begin{itemize}
		\item Formo k cluster in modo da assegnare ad ogni record il suo centroide più vicino. E' ovviamente un meccanismo esclusivo.
		\item Calcolo il nuovo centroide per ogni cluster .
		\item Mi fermo fino a quando il centroide non cambia più.		 
	\end{itemize}
	
	
\end{itemize}

Come notiamo va esplicitato il numero di cluster prima dell'esecuzione dell'algoritmo,  valori di partenza differenti possono fornire risultati molto diversi. La scelta del numero di cluster è dunque un ambito da tenere fortemente in considerazione.

L'algoritmo delle K-medie non è vincolato ad utilizzare la distanza Euclidea ma possiamo utilizzare le diverse misure di prossimità utilizzate in precedenza. In particolare:

\begin{itemize}
	\item \textbf{Manhattan:} in questo caso utilizziamo le mediane come centroidi, l'obiettivo è quindi \textit{minimizzare la somma delle $L_{i}$ distanze dei record rispetto al centroide del cluster a cui appartiene.}
	\item \textbf{Squared Euclidea:} in questo caso utilizziamo le medie come centroidi, l'obiettivo è quindi \textit{minimizzare la somma delle $L_{i}$ distanze dei record rispetto al centroide del cluster a cui appartiene.}
	\item \textbf{Cosine:} in questo caso utilizziamo le medie come centroidi, l'obiettivo è quindi \textit{massimizzare la somma delle cosine similarity dei record rispetto al centroide del cluster a cui appartiene.}
\end{itemize}


Elenchiamo tutta una serie di problematiche relative all'utilizzo dell'algoritmo delle K-medie come algoritmo di clustering.
\begin{itemize}
	\item\textit{Scelta dei centroidi iniziali:} è una fase fondamentale e influenza in modo pesante le performance dell'algoritmo in generale. Se optiamo infatti per una scelta random dei centroidi iniziali possiamo avere cluster molto diversi. Vi sono diverse alternative per ovviare a questo problema quali il clustering gerarchico.
	\item \textit{Complessità spaziale e temporale:} questi sono due punti a favore del K-medie. In particolare occupa pochissimo spazio in quanto vengono salvate solo le posizioni dei centroidi. Inoltre si tratta di un algoritmo abbastanza rapido in quanto è lineare rispetto al numero di istanze considerate. 
	\item \textit{Cluster vuoti:} bisogna tenere in considerazione che può capitare che. un cluster sia vuoto per scelta sbagliata di centroidi iniziale (magari randomica).
	\item \textit{Presenza di outlier:} Questi elementi creano grossi problemi nel calcolo della media delle osservazioni di un cluster. Risulta efficiente però nella ricerca di outlier in quanto verranno identificati come cluster di singleton. Risulta spesso utile rimuoverli.
\end{itemize}

Elenchiamo ora una serie di limiti riferiti alla ricerca dei cluster con l'algoritmo delle k-medie:
\begin{itemize}
	\item Diffiicoltà  a ricercare cluster di non forma sferica. 
\begin{figure}[H]
	\begin{minipage}[b]{0.30\textwidth}
		\centering
		\includegraphics[width=\textwidth]{clustering/pict/non_sferica_1.png}
		\caption{Cluster non sferico.}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.30\textwidth}
		\centering
		\includegraphics[width=\textwidth]{clustering/pict/non_sferica_2.png}
		\caption{Cluster sferico.}
	\end{minipage}
\end{figure}
	
	\item Difficoltà a trovare cluster con dimensioni diverse: questo problema nasce dal fatto che la distanza è fissata.
	\begin{figure}[H]
		\begin{minipage}[b]{0.30\textwidth}
			\centering
			\includegraphics[width=\textwidth]{clustering/pict/distanza_fissata_1.png}
			\caption{Distanza non fissata.}
		\end{minipage}
		\hfill
		\begin{minipage}[b]{0.30\textwidth}
			\centering
			\includegraphics[width=\textwidth]{clustering/pict/distanza_fissata_2.png}
			\caption{Distanza fissata.}
		\end{minipage}
	\end{figure}
	\item Difficoltà a indentificare cluster di diversa densità: questo prblema nasce dal fatto che i cluster possono avere dimensioni nettamente diverse.
	\begin{figure}[H]
		\begin{minipage}[b]{0.30\textwidth}
			\centering
			\includegraphics[width=\textwidth]{clustering/pict/densita_1.png}
			\caption{Densità differenti.}
		\end{minipage}
		\hfill
		\begin{minipage}[b]{0.30\textwidth}
			\centering
			\includegraphics[width=\textwidth]{clustering/pict/densita_2.png}
			\caption{Densità simili.}
		\end{minipage}
	\end{figure}
\end{itemize}
Come possiamo notare queste tre problematiche ci portano a considerare cluster notevolmenti differenti. Esiste un modo di risolvere parzialmente queste problematiche: \textit{accettare un clustering che rompe i cluster naturali in un numero variabile di sottocluster.}

Forniamo un rapido elenco rùche riassume punti di forza e debolezze dell'algoritmo delle k-medie:
\begin{itemize}
	\item Semplice e si applica a tanti tipi di dati.
	\item Abbastanza efficiente anche se vengono ripetute molte cose.
	\item Vi sono varianti più efficienti e meno problmatiche come ad esempio l'algoritmo \textit{bisecting k-means.}
	\item Non adatto a tutti i tipi di dat e non può gstire cluster non sferici, con size e densità diverse.
	\item Problemi con outliers.
	\item Strettamente legato al concetto di centroide. Vi sono delle varianti che utilizzano il \textit{medoide} e che sono più efficienti.
	
	
\end{itemize}

\paragraph{Fuzzy C-means}
Se gli elementi sono distribuiti in gruppi ben separati il clustering è semplice e vengono messi in cluster disgiunti. In molti casi succede però che i record non possano essere partizionati in cluster ben separati, nasce quindi un'incertezza arbitraria nell'assegnare gli oggetti ad un particolare cluster. Questa problematica si riflette infatti sulla scelta dell'assegnazione degli elementi di frontiera al rispettivo cluster.

\begin{figure}[H]
	\centering
	\includegraphics[height=0.4 \linewidth]{clustering/pict/frontiera.png}
	\caption{Differenza tra cluster ben separati e non ben separati.}
\end{figure}
Per ovviare al problema ogni osservazione viene considerata come appartenente ad ogni cluster ma con un peso diverso che indica il grado con cui un oggetto appartiene ad un ogni cluster. In particolare $w_{ij}$ è il \textit{peso} con cui l'$i$-esimo oggetto appartiee al $j$-esimo cluster.

Assumiamo di avere un dataset di $m$ oggetti/record, dove ogni oggetto è associato ad un numero dato di attributi continui.

\begin{defn}
	Si definisce \textbf{Fuzzy pseudo-partition} l'assegnazione dei valori dei pesi $w_{ij}$ con i seguenti vincoli:
	\[\sum_{j = 1}^{K}w_{ij} = 1 \quad i=1,2,...,m\]
	\[0 < \sum_{i = 1}^{K}w_{ij} <  m \quad j=1,2,...,K\]
\end{defn} 

Ovviamente per come sono definiti i cluster \textit{non ammetto cluster a dimensione nulla.} Il Fuzzy C-means \textit{produce dunque un clustering che risalta l'indicazione del grado per cui un oggetto appartiene ad ogni cluster.} Per come è definito ha gli stessi punti di forza e debolezze dell'algoritmo delle k-medie sebbene sia computazionalmente più pesante.

\paragraph{Modelli a mistura}
I modelli a mistura considerano i dati come insiemi di osservazioni da una mistura di diverse distribuzioni di probabilità.  In sostanza si possono considerare le misture come distribuite secondo delle normali con uguale varianza ma medie diverse (curve di livello). 

\begin{figure}[H]
	\centering
	\includegraphics[height=0.3 \linewidth]{clustering/pict/mixture.png}
	\caption{Curve di livello nei modelli a mistura.}
\end{figure}

Supponiamo di avere uno spazio a 3 componenti, il processo di generazione delle curve di livello è il seguente:
Supponiamo spazio di 3 componenti, processo di generazione:
\begin{enumerate}
	\item Seleziono una delle 3 componenti.
	\item Estraggo un campione dalla componente selezionata.
	\item Ripeto 1 e 2 m volte per ottenere il dataset
\end{enumerate} 

Definisco inoltre le seguenti grandezze:



\begin{itemize}
	\item $p(\bar{x}|\Theta) = \sum_{j=1}^{K} w_j p(\bar{x}|\theta_j)$: probabilità associata all'oggetto x.
	\item $w_j$: probabilità della j-esima componente.
	\item $p(\bar{x}|\theta_j)$: probabilità di estrarre x dalla j-esima componente.
	\item $\theta_j$: parametri associati a j.
\end{itemize}

Il processo consiste nel partire dai dati e ridurli nelle misture significative.
Esiste un classe di algoritmi utilizzata per la risoluzione di questo problema che è l'\textbf{Expectation Maximization}. Si tratta di algoritmi che sono diversi in base alla loro applicazione. 

\begin{figure}[H]
	\centering
	\includegraphics[height=0.3 \linewidth]{clustering/pict/expectation_maximization.png}
	\caption{Risultato auspicabile.}
\end{figure}
Questa classe di algoritmi presenta però diversi svantaggi:

\begin{itemize}
	\item L'apprendimento risulta essere molto lento.
	\item Non è pratico per i modelli con un gran numero di componenti.
	\item Non funziona bene quanto i cluster conengono  poche osservazioni.
	\item Non funziona bene quando gli oggetti sono co-lineari.
\end{itemize}

Questi algoritmi presentano però anche una serie di vantaggi non indifferente rispetto a quelli delle k-medie:
\begin{itemize}
	\item Sono più generali rispetto alle k-medie e alle fuzzy perché usano distribuzioni di vario tipo, possono trovare cluster di diversa grandezza e di forma ellittica.
	\item Disciplina il metodo di eliminazione delle complessità associate ad alcuni tipi di dati.
\end{itemize}

\paragraph{Mappe di Kohonen o SOMs}
E' una struttura feedforward per algoritmi di clustering, in particolare viene imposta un'organizzazione topografica dei \textit{neuroni} (centroidi). Durante il processo di training la SOM usa ogni oggetto per aggiornare il centroide più vicino e i centroidi che sono più vicini nell'ordinamento topografico.
La differenza principale tra gli algoritmi precedenti è che i centroidi in questo caso \textit{hanno una predeterminata relazione di ordinamento topografico.}

In particolare si ha che i centroidi che sono più vicini l'uno all'altro nella griglia sono più strettamente correlati rispetto ai centroidi che sono lontani.
\begin{figure}[H]
	\centering
	\includegraphics[height=0.3 \linewidth]{clustering/pict/som.png}
	\caption{Girglia di una SOM.}
\end{figure}

A causa di questo vincolo i centroidi di una griglia bidimensionale possono essere visti come giacenti su una \textit{superficie bidimensionale che prova ad adattare i dati multidimensionali il meglio possibile.}

Riassumiamo ora i passaggi 	con cui una SOM crea i cluster:
\begin{enumerate}
	\item \textit{Inizializzazione:} seleziono i centroidi assocaiti a ciascun neurone
	\item \textit{Competizione:} i neuroni competono tra loro per ottenere l'istanza. Ogni neurone fa un'offerta diversa per vincere quell'oggetto e la proposta è proporzionale alla distanza del neurone rispetto all'istanza considerata. Viene poi proclamato il vincitore e gli viene assegnata l'istanza (in base a una misura di performance).
	\item \textit{Collaborazione:} una volta che un neurone vince distribuisce il suo vantaggio ai vicini (processo di premi-punizioni) aggiustando i centroidi.
	\item \textit{Aggiornamento:} il centroide del vincitore e dei vicini vengono aggiornati usando l'istanza corrente.
\end{enumerate} 

Il punto di forza maggiore di questo algoritmo è l'\textit{Interpretabilità:} i cluster che sono vicini sono più correlati tra loro rispetto ai cluster che non sono vicini. Questo aspetto facilita enormemente l'interpretazione e la visualizzazione dei cluster.

Questo algoritmo soffre però anche di alcune limitazioni abbastanza importanti:
\begin{itemize}
	\item L'utente deve  scegliere un gran numero di parametri quali:funzione di vicinato, tipo di griglia e numero di centroidi. Questa scelta influenza molto le performance.
	\item Non sempre un raggruppamento identifica un singolo cluster naturale (solitamente dopo viene applcato un k-medie sui centroidi trovati).
	\item Mancanza di una specifica funzione oggetto: questo rende molto difficile paragonare i risultati.
	\item \textit{Non è garantita la convergenza.}
\end{itemize}

\subsubsection{Clustering Gerarchico}
Passiamo ora alla descrizione della seconda classe di algoritmi di clustering.
In particolare un clustering gerarchico può essere:
\begin{itemize}
	\item \textit{Agglomerativo}: si considerano inizialmente tutti gli oggetti come cluster individuali, ad ogni step vengono unite le coppie di step più vicine. Un processo di questo tipo richiede la definizione di prossimità tra i cluster.
	 
	\item \textit{Divisivo}: parte da un cluster unico che include tutti gli oggetti e passo passo divide il cluster in cluster di singleton (oggetti individuali). Abbiamo bisogno di decidere quale cluster splittare ad ogni passo e come splittare.
\end{itemize}

\paragraph{Clustering gerarchico agglomerativo} Concentriamo la nostra attenzione su questo tipo di clustering perché sono i più comuni. 
La soluzione che si ottiene è il \textit{dendrogramma} del clustering effettuato. Il deindogramma serve a visualizzare sia le relazioni tra i cluster e i sottocluster sia l'ordine in cui sono stati divisi o agglomerati i vari cluster. Questo tipo di algoritmo è computazionalmente molto pesante.

\begin{figure}[H]
	\centering
	\includegraphics[height=0.25 \linewidth]{clustering/pict/cluster_aggl.png}
	\caption{Clustering gerarchico sotto forma di deindrogramma}
\end{figure}

Diamo ora un rapido riassunto dei passaggi dell'algoritmo:

\begin{enumerate}
	\item Calcolare la matrice di prossimità (se necessario).
	\item \textbf{Ripeti:}
	\begin{itemize}
		\item Unione di due cluster vicini.
		\item Aggiorna la matrice di prossimità (o delle distanze) che riflette la prossimità tra il nuovo cluster e il cluster originale		.
	\end{itemize}
	\item \textbf{Finché} rimane un solo cluster.
\end{enumerate}

Si pone ora il problema di calcolare la prossimità tra due cluster . Ci sono in particolare tre modi:
\begin{itemize}
	\item  \textit{Min or Single Linkage}: minore distanza tra tutte le possibili coppie di elementi presenti nei due cluster, per come è definita è ovviamente la soluzione più ottimista.
	\item \textit{Max or Complete Linkage}: maggiore distanza tra tutte le possibili coppie di elementi presenti nei due cluster, è una soluzione più robusta o pessimista.
	\item \textit{Group Avarage or Avarage Linkage}: media distanza tra tutte le possibili coppie di elementi presenti nei due cluster, è una soluzione abbastanza usata in quanto media le distanze.
	\item \textit{metodo di Ward}: assume che i cluster siano rappresentati dai centroidi. Misura la prossimità tra due cluster come l'incremento della somma di scarti quadratici che risulta dalla fusione di due cluster.
\end{itemize}
	\begin{figure}[H]
	\begin{minipage}[b]{0.30\textwidth}
		\centering
		\includegraphics[width=\textwidth]{clustering/pict/sing_linkage.png}
		\caption{ \textit{Single}}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.30\textwidth}
		\centering
		\includegraphics[width=\textwidth]{clustering/pict/complete_linkage.png}
		\caption{\textit{Complete}}
	\end{minipage}
	\begin{minipage}[b]{0.30\textwidth}
	\centering
	\includegraphics[width=\textwidth]{clustering/pict/average_linkage.png}
	\caption{\textit{Average}}
\end{minipage}
\end{figure}

Le caratteristiche chiave di questi algoritmi sono le seguenti:
\begin{itemize}
	\item \textit{Mancanza di una funzione globale di oggetto:} per questo motivo non può risolvere problemi di ottimizzazione globale.
	\item \textit{Abilità di gestire cluster di diverse dimensioni:} può gestire cluster con dimensioni differenti, i cluster possono anche essere pesati o meno.
	\item \textit{Le decisioni di unione sono definitive:} non si può tornare indietro rispetto alla decisione (approccio greedy).
\end{itemize}

Le problematiche più importanti sono invece:
\begin{itemize}
	\item Computazionalmente molto pesante e richiede molto spazio.
	\item Decisioni di unione di cluster sono subottimali quindi creano rumore soprattutto per dati documentali. 
\end{itemize}

\subsubsection{Density-Based Clustering}
Passiamo ora alla descrizione della classe di algoritmi basati sulla densità. In particolare le tecniche di clustering basate sulla densità si occupano di \textit{trovare regioni con un'alta densità di oggetti circondati da regioni con una bassa densità di oggetti.}
Passiamo ora a parlare di un algoritmo specifico, il \textit{DBSCAN.}

\paragraph{DBSCAN} E' un semplice ma efficace  algoritmo density-based che mostra un gran numero di concetti fondamentali tipici dell'approccio density based. Ci sono diversi metodi per definire la densità, in particolare noi descriviamo il metodo center-based.
\begin{defn}
	Si definisce \textbf{densità} il numero di oggetti presenti all'interno di un raggio fissato (Eps) rispetto ad un oggetto.
\end{defn} 

Come facilmente intuibile la densità può variare fortemente al variare del raggio.
\begin{figure}[H]
	\centering
	\includegraphics[height=0.25 \linewidth]{clustering/pict/density.png}
	\caption{Densità rappresentata graficamente}
\end{figure}

Definita la densità in questo modo, i punti vengono classificati in tre tipologie diverse:
\begin{itemize}
	\item \textit{Core point}: stanno all'interno della regione ad elevata densità. In particolare un punto viene definito così se il numero di punti nel suo vicinato eccede un certo valore di soglia (MinPts) pre impostato dall'utente. 
	\item \textit{Border point} : un punto che include nel suo vicinato un core point, quindi lui si considera un vicino di core point.
	\item \textit{Noise point}: non è né un core point né un border point.
\end{itemize}

Come negli altri casi presentiamo i passaggi dell'algoritmo in maniera schematica:
\begin{enumerate}
	\item Etichettare tutti i core, border, noise point.
	\item Eliminare i noise point.
	\item Collegare con un ponte tutti i core point che sono all'interno dei rispettivi Eps.
	\item Trasformare ogni gruppo di core point connessi in cluster separati.
	\item Assegnare  ciascun border point al cluster associato al core point più prossimo.
\end{enumerate}

Come detto in precedenza la scelta dei parametri \textit{MinPts} e \textit{Eps} è fondamentale in quanto impatta fortemente sull'esecuzione di tutto l'algoritmo. Mostriamo ora invece quali sono le differenze principali tra l'algoritmo DBSCAN e l'algoritmo K-means:
\begin{itemize}
	\item DBSCAN and K-Means assign objects to a single cluster, but K-Means assigns all
	objects while DBSCAN can discard noise objects
	\item DBSCAN can handle clusters of different sizes and shapes and it is not strongly
	affected by noise or outliers. K-means has difficulties with non-globular clusters and
	clusters of different sizes. Both algorithms perform poorly when clusters have widely
	differing densities
	\item K-Means can only be used for data that has a well defined centroid, such as mean or
	median. DBSCAN requires that its definition of density, which is based on the
	traditional Euclidean notion of density be meaningful for the data
	\item K-Means can be applied to sparse, high-dimensional data, such as document data.
	DBSCAN typically performs poorly for such data because the traditional Euclidean
	definition of density does not work well for high-dimensional data
	\item DBSCAN makes no assumption about the distribution of the data. The basic K-means
	is equivalent to a statistical clustering approach (Mixture Model) that assumes all
	clusters come from spherical Gaussian distributions with different means but the
	same covariance matrix
	\item DBSCAN and K-Means both look for clusters using all attributes, that is, they do not
	look for clusters that may involve only a sub-set of the attributes
	\item K-Means can find clusters that are not well separated, even if they overlap, but
	DBSCAN merges clusters that overlap
	\item K-Means has complexity O(m) while DBSCAN is O(m2) (except for low dimensional
	Euclidean data.)
	\item DBSCAN produces the same set of clusters from one run to another while K-Means,
	which is typically used with random initialization of centroids, does not
	\item DBSCAN automatically determines the number of clusters, for K-Means the number
	of clusters needs to be specified as a parameter
\end{itemize}

Vi sono altri algoritmi di density-based che non tratteremo in questo momento quali: \textit{grid-based clustering}, \textit{subspace clustering}, \textit{kernel density function}.

\begin{comment}

\underline{DBSCAN vs K-medie}:

\noindent


\subsubsection{Graph-based Clustering Algorithm}
Gi\`a gli algoritmi a rappresentazione gerarchica possono essere visti come graph-based. 

Sparsificare: tagliare, rimuovere certi archi che non superano la verifica di una certa condizione (Es. tutti gli archi che non superano una certa soglia, oppure un concetto di vicinato). 

Definire una misura di similarit\`a tra due oggetti basati sul numero di nearest neighbors. 

Definire oggetti core e costruire cluster attoro ad essi. Necessita di concetti di densit\`a e prossimit\`a.

Utilizza l'informazione nel grafo di prossimit\`a per fornire una maggiore valutazione di specialit\`a in cui due cluster dovrebbero essere uniti. 

Grafo di prossimit\`a stabilisce dei pesi tra i vari nodi che \`e il costo da percorrere tra un nodo ed un altro, \`e rappresentazione della matrice delle distanze. 

I legami di similarit\`a tra nodi vengono suddivisi in \textit{sufficientemente vicini} (superano il threshold) e \textit{non vicini} quindi con vicinanza minore rispetto al threshold. Vengono quindi rimossi gli archi che non superano il threshold, questo significa \textit{sparsificare} la matrice. Viene cos\`i generato il grafo di prossimit\`a sparsificato.

Definiamo quindi il \textbf{K-Nearest Neighbor}\\
se fisso k = 3 scelgo per ogni riga della matrice delle prossimit\`a i primi 3 valori degli archi per come vicinato. 

2 modi di procedere: threshold o k neighbors.

La sparsificazione riduce anche di molto il numero di elementi del dataset. Il clustering lavora molto meglio in quanto si lavora in un concetto di vicinanza. Algoritmi su grafi partizionati possono essere usati. 

\subsubsection{Altri algoritmi}
Non sempre purtroppo la sparsificazione del grafo porta ad un grafo sconnesso. L'idea \`e di ciclare la sparsificazione finch\`e non otterr\`o dei grafi partizionati ben definiti. 

\textbf{Minimum spanning tree} o albero di supporto a costo minimo:
\begin{itemize}
	\item non ha cicli
	\item contiene tutti i nodi dell'albero
	\item ha il minimo costo totale dei pesi associati ai suoi alberi di tutti i possibili spanning tree
\end{itemize}
\noindent
\textbf{MST divisive hierarchical clustering Algorithm}
\begin{enumerate}
	\item Calcola il MST per grafo di dissimilarit\`a
	\item \textbf{Ripeti}
	\item Crea un nuovo cluster per rompere il collegamento corrispondente alla pi\`u grande dissimilarit\`a
	\item \textbf{Finch\`e} rimangono solo cluster singoli
\end{enumerate}
\noindent
Algoritmo \textbf{Opossum}:
\begin{enumerate}
	\item Caclola il grafo di similarit\`a sparso
	\item partiziona il grafo in k componenti distinte (cluster) usando METIS
\end{enumerate}
\noindent
Altro algoritmo \textbf{Camaleon}.

Quando si visualizza il dendrogramma, si possono visualizzare le distanze e solitamente ha una forma a "gomito" la soluzione ottimale \`e solitamente quella che precede la salita finale del comito. Un buon dendrogramma ha grandi salti prima di aggregare insieme (salti grandi significa che se raggruppo genero un errore importante quindi \`e positivo per la valutazione del clustering). 

\subsection{Clustering Evaluation}

L'efficienza di un algritmo di clustering viene valutato tramite:
\begin{itemize}
	\item similarit\`a
	\item esclusivit\`a dei clustering o misure fuzzy di sovrapposizione
	\item completo o parziale clustering
\end{itemize}

Innanzitutto scelgo alcuni algoritmi candidati da voler implementare in base alla tipologia di dati che ho. L'utilit\`a di utilizzarne diversi e poterli confrontare. Bisogna impostare un'insieme di esperimenti equi per poterli valutare correttamente (stessi dati, chiare differenze di parametri ecc). 
Il problema \`e che \`e molto complesso valutare i cluster visto che non so esattamente cosa sto cercando. 

I problemi grossi nella valutazione sono:
\begin{itemize}
	\item determinare la tendenza dei cluster (che non siano fatti a caso)
	\item determinare il numero corretto di cluster (qualunque cosa significhi)
\end{itemize} 

abbiamo 3 diversi tipi di indici:
\begin{itemize}
	\item \textbf{Esterni o supervisionati}: misure che estendono i cluster scoperti con alcune strutture esterne (sapendo circa la verit\`a sul problema)
	\item \textbf{Interni o non supervisionati}: misure di similarit\`a di cluster in particolare le strutture in base a informazioni esterne, possono essere:
	\begin{itemize}
		\item Misure di coesione: determina la connessione tra elementi di un cluster
		\item misure di separazione: quanto sono ben distiniti i cluster 
	\end{itemize}
	\item \textbf{Relativi}: compara differenti clustering
\end{itemize}

\subsubsection{Esterni o supervisionati}
$P = \{P_1, ..., P_R\}$ partizione di un dataset di m (oggetti record) in R categorie, possono essere partizionati con algoritmi in k cluster $C= \{C_1, ..., C_K\}$.

Casi:
\begin{enumerate}
	\item (\textbf{a}) x e y appartengono allo \underline{stesso} cluster di C e alla \underline{stessa} categoria di P
	\item (\textbf{b}) x e y appartengono allo \underline{stesso} cluster di C e a \textit{differenti} categorie di P
	\item (\textbf{c}) x e y appartengono a \textit{differenti} cluster di C e alla \underline{stessa} categoria di P
	\item (\textbf{d}) x e y appartengono a \textit{differenti} cluster di C e a \textit{differenti} categorie di P
\end{enumerate}

Numero totale di coppie che si possono formare:
\[ M = \frac{m x (m-1)}{2} = a + b + c + d\]

Rand: \[R = \frac{a + d}{M}\] con $R \in [0,1]$

Jaccard: \[J = \frac{a}{a + b + c}\] con $J \in [0,1]$

Fowlkes and Mallows: \[FM = \sqrt{\frac{a}{a + b} x \frac{a}{a + c}}\]

$\Gamma$ statistics

\subsubsection{Interni o non supervisionati}

Molte misure di validit\`a del partizionamento dei cluster si basano sulla \textbf{coesione} e sulla \textbf{separazione}. Noi di base vogliamo un valore di coesione alto ed un valore di separazione basso.

Ci si basa sul concetto di grafo (graph-based), la coesione di un cluster pu\`o essre definita come a somma dei pesi degli archi inn un grafo di prossimit\`a che connette i punti all'interno del cluster. 

\[ cohesion(C_i) = \sum_{x, u \int C_i} proximity(x,y) = \sum_{x y \in C_i} similarity(x,y)\]

La similarit\`a \`e quindi inversamente proporzionale alla dissimilarit\`a. 
La separazione \`e valutata secondo la somma dell prossimit\`a di nodi che non appartengono allo stesso cluster.

Per i cluster prototype-based la coesione \`e valutata rispetto al centroide che ne rappresenta il prototipo; la separazione \`e misurata sulla prossimit\`a del centroide o prototipo del cluster. Altro metodo per la separazione \`e la valutazione rispetto al centroide generato dai due cluster.

La validit\`a \`e valutata in questo modo: $overall validity = \sum_{i = 1}^{K} w_i \cdot validity(C_i)$, dobbiamo decidere per\`o il peso da applicarvi ($W_i$). 


Importante capire che alle volte separare un cluster potrebbe migliorare le misure di coesione, quando ho valori di coesione bassi.

Se invece ho valori di coesione buoni ma valori di separazione alti allora provando ad unirli si raggiungono cluster pi\`u coesi. 

La valutazione che sta all'interno del cluster contribuisce maggiormente informazione alla coesione e separazione, invece per le osservazioni pi\`u esterne queste valutazioni possono non essere cos\`i utili. Pertanto \`e stato definito l'\textbf{indice di Silhouette}. questo indice combina coesione e separazione per l'i-esimo oggetto e si definisce in questo modo:

\[s_i = \frac{b_i - a_i}{max(a_i,b_i)} \in [-1,+1]\]

dove $a_i$ \`e la media della distanza tra l'oggetto i-esimo e gli altri oggetti nel cluster, $b_i$ la minima distanza media dell'oggetto i-esimo a tutti gli altri oggetti di ogni custer diverso dal cluster di partenza dell'oggetto.

(es. se $a_i = 0$ allora $s_i = 1$ perch\`e si trova nel cluster migliore. se ha invece valore $< 0$ significa che se avessi assegnato ad un altro cluster avrei avuto un indice di silhouette pi\`u alto)

L'avarage Silhouette coefficient di un cluster da un valore di silhouette medio 

Per il clustering gerarchico viene definito il Cophenetic Correlation Coefficient: 
Si considera la matrice di prossimit\`a e la matrice cophenetica.
Cophenac matrix \`e la matrice che tiene conto delle distanze tra coppie di osservazioni raggruppate all'interno di uno stesso cluster. Il valore varia da $[-1,1]$. Pi\`u il valore \`e vicino a 1 meglio l'algoritmo gerarchico fitta.

\subsubsection{Paradigma di validit\`a}
Rispondere alla domanda se il nostro dataset tende ad essere clusterizzabile.

Vengono considerati criteri interni ed esterni legati a metodi statistici e test di ipotesi. 

Se non ci fosse nessuna struttura dei dati (ipotesi NULLA). 

\begin{itemize}
	\item Random Position Hypothesis in tutte le location di m osservazioni in specifiche regioni n-dimensionali sono equamente buone $\rightarrow$ per ratio data
	\item Random Graph Hyothesis la matrice di prossimit\`a ha rank equamente buoni $\rightarrow$ per prossimit\`a ordinali tra coppie di osservazioni
	\item Random Level Hypothesis tutte le permutazioni delle label di m osservazioni sono equamente buone $\rightarrow$ per tutti i tipi di dati
\end{itemize}

A questo punto valuto in base alla distribuzione sotto l'ipotesi nulla e ci calcolo l'indice. Se il valore dell'indice \`e all'interno del quantile fissato ($\alpha$) posso affermare che ci sia struttura nel dataset, se invece l'ipotesi \`e rigettata allora si sa che il clustering sviluppato non ha struttura pertanto pu\`o essere mostrata al proprio interlocutore l'algoritmo sviluppato. 

\subsubsection{Selezione del numero di cluster}
A senso porsi questa domanda se si \`e prima rilevato che vi \`e struttura di clustering. 

I criteri \textbf{relativi} \`e un uso particolare delle misure di clsustering per comprendere il numero ottimale di cluster. Si proietta lo spazio delle osservazioni in uno spazio di dimensione minore e da questo si utilizzano delle tecniche di valutazione.

Gli indici principali sono:
\begin{itemize}
	\item Calinski and Harabasz: seleziono il numero di k che ottimizza il numero di cluster
	\item Dunn: simile a quello sopra
	\item Devies-Bouldin: minimizzo K per l'ottimo numero di cluster
\end{itemize}

Misure miste di probabilit\`a model-based clustering sono:
\begin{itemize}
	\item Akaike Information Criterion (AIC)
	\item Minimum Description Length (MDL)
	\item Bayesian Information Criterion (BIC)
\end{itemize}

Per l'algoritmo di clustering richiede un input di numero di clsuter K dagli utenti una sequenza di strutture di clustering ottenute eseguendo l'algoritmo motlre volte dove K spazia da $K_{min}$ a $K_{max}$.

\end{comment}