\relax 
\babel@aux{italian}{}
\babel@aux{italian}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Lezione 1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Introduzione}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Data Types}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Data exploration}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Missing replacement}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Data Preprocessing}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Lezione 2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Lezione 3}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Classification}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces modello di matrice di confusione}}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Tecniche di classificazione}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Euristici}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces modello decision tree}}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Regressione Logistica Binomiale}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Lezione 4}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Support Vector Machines}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Reti di neuroni artificiali}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Lezione 5}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Naive bayes}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces rete naive bayes}}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Reti bayesiane}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces rete bayesiana generalizzata}}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Tree-augmented Naive Bayes}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces tree-augmented Naive bayes}}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Summary}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces comparazione tra i principali modelli}}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Lezione 6}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Performance di un modello di classificazione}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Accuratezza}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Velocit\`a}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}Holdout}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Lezione 7 - Comparing Classifiers}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Intervallo di confidenza}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Class Imbalance}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Lezione 8}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Matrice di costo}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces legame tra matrice di confusione (sn) e matrice di costo (dx)}}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Cumulative Gains}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces cumulative gains evidenziando un punto di alto valore aggiunto}}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Lift Chart}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces da cumulative gain a lift chart}}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Curva ROC}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces curva ROC legata a dataset variabili (pi\`u reale)}}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces confronto ROC da due modelli}}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Lezione 9 - Feature Selection}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Filter - Wrapper, due differenti approcci per feature selection}}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces vantaggi e svantaggi tra tecniche uni-variate e multi-variate}}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces vantaggi e svantaggi tra tecniche uni-variate e multi-variate}}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Lezione 10 - Classificazione NON binaria}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}One-Vs-All}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Train/validation/test e feature selection}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {11}Clustering}{31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.0.1}Tipi di clustering}{31}}
\@writefile{toc}{\contentsline {paragraph}{Partizionale}{31}}
\@writefile{toc}{\contentsline {paragraph}{Gerarchica}{31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.0.2}Differenti nozioni di cluster}{32}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.0.3}Metodologia}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {12}Lezione 11 - Proximity}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces vantaggi e svantaggi tra tecniche uni-variate e multi-variate}}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1}Misure della distanza}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces vantaggi e svantaggi tra tecniche uni-variate e multi-variate}}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces esempio di prototype-based clustering}}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2}Clustering Algorithms}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3}K-medie}{36}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.3.1}Algoritmo}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {13}Lezione 12 - Clustering Algorithm}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1}Fuzzy C-means}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2}Modelli a mistura}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3}Mappe di Kohonen o SOMs}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {14}Lezione - Clustering Algorithm}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1}Clustering Gerarchico}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces da clustering gerarchico a dendrogramma}}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2}Density-Based Clustering}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3}Graph-based Clustering Algorithm}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4}Altri algoritmi}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {15}Lezione - Clustering Evaluation}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.1}Esterni o supervisionati}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2}Interni o non supervisionati}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3}Paradigma di validit\`a}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4}Selezione del numero di cluster}{49}}
\@writefile{toc}{\contentsline {section}{\numberline {16}Lezione - Analisi di associazione}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1}Generazione di frequent itemset e generazione regole}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2}Algoritmo apriori}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3}Rule Generation}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces gerarchia dei frequent itemset}}{54}}
