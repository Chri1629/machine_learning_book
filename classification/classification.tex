\section{Classification}

\subsection{Introduction (*)}
Entriamo nel dettaglio della componente di validazione nei modelli di classificazione supervisionata.
La cosa più importante è \textbf{capire cosa stiamo facendo} in quanto quando si applicano algoritmi di machine learning è facile perdersi.

Nel solito dataset dei churn vogliamo determinare se un cliente abbandonerà o meno il nostro servizio; per farlo, cerchiamo di utilizzare una particolare combinazione di attributi. Bisogna innanzitutto vedere di che tipo sono le variabili presenti nel nostro database.

Lo scopo è di essere  in grado sia di prevedere quando un cliente abbandonerà un determinato servizio, che di capire \textbf{le motivazioni} che l'hanno spinto a farlo. Questo tipo di compiti può essere portato a termine con l'utilizzo di un modello di \textbf{classificazione}. Un modello di classificazione è un modello che sfrutta alcuni attributi del dataset (\textbf{attributi esplicativi}) per prevedere un valore di un altro attributo (\textbf{attributo di classe}).

\begin{figure}[H]
	\centering
	\includegraphics[width= \linewidth]{classification/pict/class_model.png}
	\caption{processo di classificazione}
\end{figure}

Forniamo di seguito delle definizioni utili per poter parlare più nel dettaglio dei metodi di classificazione:
\begin{defn}
	Si definiscono \textbf{variabili esplicative} le variabili in input.
\end{defn}

\begin{defn}
	Si definiscono \textbf{variabili di classe}	Le variabili in output.
\end{defn}

\begin{defn}
	 Si definisce \textbf{modello di classificazione} un modello in grado di risolvere un problema di classificazione; i modelli di classificazione si dividono in:
	\begin{itemize}
		\item \textbf{Modello descrittivo}: serve come strumento di spiegazione per distinguere tra oggetti di classi diverse
		\item \textbf{Modello predittivo}: predice la classe di un record sconosciuto, pu\`o essere visto come una \underline{scatola nera} che asssegna una label di una classe al record sconosciuto
	\end{itemize}
\end{defn}

\begin{defn}
	Si definisce \textbf{classificatore} l' approccio sistematico per costruire un modello di classificazione su un dataset.
\end{defn}
La costruzione di un classificatore segue il seguente processo:
\begin{figure}[H]
	\centering
	\includegraphics[height=0.6 \linewidth]{classification/pict/class_process.png}
	\caption{Processo di creazione della classificazione}
\end{figure}

\begin{defn}
	Si definisce \textbf{training set}  l'insieme dei record in cui tutti i valori degli attributi sono noti.
\end{defn}

\begin{defn}
	Si definisce \textbf{test set} l'insieme dei record i cui valori dell'attributo di classe sono sconosciuti (o presunti tali).
\end{defn}

Si utilizza quindi un algorimo di learning (\textbf{Learner}) sul training set, passaggio definito come \textbf{Classification Model Learning}. 

\begin{defn}
	Si definisce \textbf{Inducer} l'output di questa operazione.
\end{defn}

Il ruolo dell'inducer è quello di predire il valore dell'attributo di classe per il test set. 
La chiave di tutto è la scelta dell'algoritmo learner per l'apprendimento; la scelta degli attributi esplicativi è fondamentale, alcuni di essi possono, infatti, essere solo ridondanti nel processo e generare solo rumore. Tramite l'analisi delle performace si possono valutare queste due scelte. 

Bisogna valutare le performance del modello inducer. Uno dei modi per analizzare se è efficace è la \textbf{matrice di confusione}. 

Nelle righe vi sono i veri valori delle classi, nelle colonne i valori predetti dall'inducer. Il numero di righe è uguale al numero di colonne e corrispondono al numero di classi da predire.

\begin{figure}[H]
	\centering
	\includegraphics[height=0.25 \linewidth]{classification/pict/matrconf.png}
	\caption{modello di matrice di confusione}
\end{figure}
All'interno sono identificati i risultati dei test e sono così classificati:
\begin{itemize}
	\item \textbf{TN - vero negativo}: num. di record dove AC=-1  che sono stati corretamente predetti IP=-1
	\item \textbf{FN - falso negativo}: num. di record dove AC=+1  che sono stati erroneamente predetti IP=-1
	\item \textbf{TP - vero positivo}: num. di record dove AC=+1  che sono stati corretamente predetti IP=+1
	\item \textbf{FP - falso positivo}: num. di record dove AC=-1  che sono stati erroneamente predetti IP=+1
\end{itemize}

\begin{defn}
	La misura più utilizzata come metrica di performance è l'\textbf{accuratezza}:
	
	\[accuracy = \frac{\sum_{i = 1}^{n} diagonal}{\sum_{i=1}^{n}elements} = \frac{\#CorrPrediction}{\#Prediction} = \frac{TN + TP}{TN + TP + FN + FP}\] 
	
	La misura complementare che troveremo indicata è quella dell'errore:
	
	\[error = \frac{FN + FP}{TN + FN + FP + TP} =  1 - accuracy\]
\end{defn}

\subsection{Tecniche di classificazione}

Una tecnica di classificazione è un modo sistematico di aggredire un dataset e costruire i modelli di classificazione, in particolare possiamo dividerle in 4 macro categorie:
\begin{itemize}
	\item \textbf{Euristiche}: ispezionano il suo vicinato come ad esempio: Decision Trees, Random Forest, Nearest Neighboor
	\item \textbf{Regression Based}: usa la probabilità condizionata parametrica, ad esempio: regressione logica
	\item \textbf{Separazione}: partiziona lo spazio degli attributi, fa riferimento alle Support Vector Machine e alle Artificial Neural Network
	\item \textbf{Probabilistici}: usano la formula di Bayes (Naive Bayes ecc...).
\end{itemize}

Forniremo una overview di tutte queste categorie senza trattarle nel dettaglio; partiamo dalla trattazione dei modelli \textbf{Euristici}.

\subsubsection{Decision Tree}
I modelli decision tree sono modelli di tipo euristico. 

Un \textbf{albero di decisione} ha una rappresentazione grafica precisa in cui gli elementi principali sono: nodi e archi. Il \textbf{nodo} rappresenta un sottoinsieme del dataset, gli \textbf{archi} sono usati, invece, per modellare gli output di modelli diversi di dataset.

\begin{figure}[H]
	\centering
	\includegraphics[height=0.7 \linewidth]{classification/pict/decision_tree.png}
	\caption{Modello decision tree}
\end{figure}

Gli elementi principali da cui è costituito sono:
\begin{itemize}
	\item \textbf{Root node} (nodo radice): non ha archi in ingresso ma può averne più di due in uscita
	\item \textbf{Internal nodes} (nodi interni): hanno un solo arco in ingresso e almeno due in uscita
	\item \textbf{Leaf} (nodi foglia o terminali): sono nodi interni senza archi in uscita e con esattamente un arco in ingresso.
\end{itemize}

Ogni record viene classificato partendo \textbf{dall'alto} (radice) fino \textbf{al basso} (nodi foglia).

Leggo il primo nodo (radice) in cui è presente l'indicazione di un attributo: nell'esempio se 'day charge' sia $\le 41,665$ o  $>$ e divide l'albero in due nodi. Se la risposta è vera allora mi muovo in un nodo altrimenti nell'altro. All'interno del nuovo nodo ho di nuovo la valutazione di una variabile, e proseguo così finché raggiungo un nodo foglia.
Quando si arriva ad una foglia devo fornire una risposta, ovvero conto la classe più frequente e rispondo al chiamante con essa (es. churn = n).

Il decision tree può essere usato per attributi nominali, ordinali così come per intervalli numerici e coefficienti.
 
Diverse misure possono essere usate per selezionare la politica di node splitting ottima: \textit{entropia}, \textit{indice di Gini} e il \textit{classification error}. Esso comunque dipende anche dal tipo di attributo: \textit{binario}, \textit{nominale} e \textit{continuo}. 

E' un modello che risponde sempre \textit{la classe più frequente ed è quindi inutile quando ho delle classi particolarmente sbilanciate.} Sono evidentemente test univariati, ciò che faccio è costruire iper parallelepipedi del nostro dataset. Vanno a definirsi delle rette per il cambio di classificazione: \textbf{decision boundary}.

\begin{figure}[H]
	\centering
	\includegraphics[height=0.5 \linewidth]{classification/pict/decision_boundary.png}
	\caption{Decision boundary}
\end{figure}

Il risultato di queste rette fa la differenza sulla capacità di evidenziare dove siano presenti elementi di una certa classe, devo quindi imparare a posizionare questi iperpiani: \textbf{l'obiettivo è partizionare il mio spazio in iperpiani massimizzando l'accuratezza.}

Non c'è alcuna ragione per cui io non possa usare degli splitting multipli e non solo su binari, posso farlo anche su valori nominali.

Vi sono poi diversi modelli derivati dal decision tree, come il Random Forest.

\textbf{Random forest}: è un comitato di alberi di decisione. Usa degli attributi che sono in generale sottoinsiemi degli attributi, ogni albero può avere un sottoinsieme differente di attributi. Ogni albero usa attributi in modo casuale. Ogni albero apprende a modo suo e il random forest in base a dei parametri (regione dello spazio) decide a quale albero ascoltare per la decisione.

\subsubsection{Regressione Logistica Binomiale}
La regressione logistica è un metodo di classificazione basato sulla regressione.

Assumiamo che la variabile di classe Y sia compresa tra $\{0,1\}$, allora la regressione logistica binaria calcola a \textit{posteriori} la probabilità che Y dia il valore dell'input ovvero le variabili esplicative.
Servono per risolvere problemi di regressione binaria a diversi livelli. E' applicabile ad attributi continui e con certe accuratezze anche ad attributi nominali.

Si assume che l'attributo di classe sia tale che $Y = \{0,1\}$;  il classificatore a regressione logistica binomiale calcola a posteriori la probabilità che $Y$ assuma il valore di un input esplicativo $\underline{X}$. Si cacola come segue:

\[P(Y = 0 | \underline{X}= \underline{x}) = \frac{1}{1+exp(\underline{x} \cdot \underline{w})}\]

\[P(Y = 1 | \underline{X}= \underline{x}) = \frac{exp(\underline{w} \cdot \underline{x})}{1+exp(\underline{w} \cdot \underline{x})}\]

dove $\underline{w}$ è chiamato \textit{vettore parametro.}

\begin{figure}[H]
	\centering
	\includegraphics[height=0.5 \linewidth]{classification/pict/regr_logistic.png}
	\caption{Esempio di regressione logistica}
\end{figure}

In questo modo vengono calcolate le probabilità di appartenere ad una certa classe.	 

\subsubsection{Support Vector Machines}
La tecnica Support Vector Machines è un metodo di classificazione con separazione. Lo scopo è separare o "apprendere" classi che vogliamo classificare. 

Consideriamo uno spazio bidimensionale in cui sono rappresentati $m$ record dove due attributi continui vengono misurati: $D = \{(\underline{x}_1, y_1),...,(\underline{x}_m, y_m)\}$ in cui $\underline{x}_i \in R^2$ e $y_i \in \{-1, +1\}$

L'idea è quella di tracciare una retta (se ho due sole classi) per cui definisco l'area di appartenenza di una o dell'altra classe. 

\begin{figure}[H]
	\centering
	\includegraphics[height=0.4 \linewidth]{classification/pict/svm.png}
	\caption{Esempio di una possibile retta di separazione}
\end{figure}

La retta in questione è definita dalla seguente equazione:

\[\underline{w} \cdot \underline{x} + b = w_1 x_1 + w_2 x_2 + b = 0\]

Per orientare la retta utiliziamo il vettore $\underline{w} = [w_1,w_2]$ oppure $b$. ($\underline{w}$ fa ruotare, $b$ fa traslare)\\
se la retta esiste allora l'insieme delle istanze è \textit{linearmente separabile}.

\textit{Problema}: possono esserci più (anche infinite) rette utilizzabili per effettuare la separazione, quale devo scegliere? Perché non basta trovare una retta che funziona, ma la retta migliore in quanto l'algoritmo dovrà essere testato su dati nuovi. Si crea una sorta di area grigia nella quale non so dire esattamente quale delle due classi categorizzare (\textbf{Linear Decision Boundary}).

\begin{figure}[H]
	\centering
	\includegraphics[height=0.4 \linewidth]{classification/pict/svm_rette.png}
	\caption{Diverse rette soluzioni dello stesso problema}
\end{figure}

Bisogna sostanzialmente trovare una retta che \textbf{massimizza il margine di errore}, l'\textit{optimal linear decision boundary}. 

\begin{figure}[H]
	\centering
	\includegraphics[width= \linewidth]{classification/pict/svm_boundary.png}
	\caption{Boundary}
\end{figure}

La retta $B_1$ è nettamente preferita rispetto alla retta $B_2$ in quanto ha un margine di supporto $\delta$ maggiore. 

Naturalmente per $n$ attributi bisogna trovare l'\textit{iperpiano} ottimale per dividere un determinato insieme di istanze. Matematicamente parlando si cerca di massimizzare il margine $\delta = 1 / |w|^2$, dove la retta ha la seguente impostazione: $\bar{w} \cdot \bar{x} + b = 0$. Le rette ai confini del margine sono fissate a $\bar{w} \cdot \bar{x} + b = 1$ e $\bar{w} \cdot \bar{x} + b = -1$. 

L'argomento della retta è in 2 dimensioni, però dopo aver applicato la funzione $h(\underline{x})$ quella retta diventa un piano che va a $-1$ da un lato e a $+1$ dall'altro.

Il training di una \textbf{SVM Linear Hard-margin} avviene formulando e risolvendo il seguente problema matematico: 

\[ \min_{\underline{w},b} \frac{1}{2}\underline{w} \cdot \underline{w}^T \]
\qquad s.t.
\[ y_i (\underline{w} \cdot \underline{x}_i + b) \ge 1 \quad \forall i = 1, ..., m\]

È un problema di programmazione quadratica con vincoli lineari i quali devono essere risolti con tecniche numeriche speciali.

Per trovare la retta devo minimizzare l'inverso del margine $\delta$; per farlo bisogna imporre dei vincoli per ogni attributo. In particolare,  se $\underline{w}$ e $\underline{x}$ sono \textit{concordi in segno} (positivo o negativo) allora classifica perfettamente perché il risultato è $>= 1$. Questi vincoli garantiscono che tutti i casi del dataset siano classificati correttamente e tra tutti i casi che classificano correttamente scelgo quello che massimizza il margine. 

Non risolveremo questa formula di ottimizzazione in modo diretto, ma la sua formulazione \textbf{duale}. In ogni caso questa formulazione funziona bene se il problema è  \textit{linearmente separabile}.

Nei casi in cui il problema\textbf{non} è linearmente separabili, non esiste \textbf{mai} una retta in grado di separare correttamente le classi. In questi casi la formulazione precedente non ammette soluzione, perché per alcuni dati i vincoli non ammettono soluzione. 

Si introduce allora la \textbf{Linear Soft-margin}:

\[ \min_{\underline{w},b,\underline{\epsilon}} \frac{1}{2}\underline{w} \cdot \underline{w}^T + \Delta \sum_{i=1}^{m} \epsilon_i \]
\qquad s.t.
\[\forall_{i=1}^m :  y_i (\underline{w} \cdot \underline{x}_i + b) \ge 1 - \epsilon \] \[\forall_{i=1}^m : \epsilon \ge 0\]

Le $\epsilon$ devono essere non negative (variabili di slack), se utilizzo questo parametro  per ammettere un errore allora esiste almeno una retta che risolve il problema di ottimizzazione. Ho sostanzialmente \textit{rilassato} il problema di ottimizzazione, in particolare i vincoli. Graficamente parlando ciò si traduce nel traslare degli elementi di una classe diversa dalla regione di appartenenza verso la regione della classe di quell'elemento.
E' molto importante notare che vettori di supporto sono quelle osservazioni che sono sul bordo del margine

\textbf{Problema}: La separazione si presta ad essere eseguita solo con una funzione \textbf{non lineare}.
 
\textbf{Soluzione}: Si va a cercare una trasformazione che porti dallo spazio originale in uno spazio delle features in cui posso applicare una separazione lineare. Nel nuovo spazio sono in grado di separare il nuovo dataset in due diverse classi (vedi immagine che segue). 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9 \linewidth]{classification/pict/svm_nonlinear.png}
	\caption{Applicazione di una separazione non lineare}
\end{figure}

In questo modo posso sfruttare tutta la metodologia precedente ma in uno spazio "controllato". Bisogna trovare però una trasformazione $\phi(x)$ che mappi X in F.
La nuova Linear Deision Boundary nello spazio delle feature F è definito dalla seguente equazione: 
\[\underline{w} \cdot \phi(\underline{x}) + b = 0\]

Il modello diventa:

\[ \min_{\underline{w},b} \frac{1}{2}\underline{w} \cdot \underline{w}^T \]
\qquad s.t.
\[ y_i (\underline{w} \cdot \phi(\underline{x}_i) + b) \ge 1 \quad \forall i = 1, ..., m\]

Per l'algoritmo di apprendimento utilizziamo sostanzialmente delle funzioni kernel che sono del tipo:
 \[K(\underline{u}, \underline{v}) = \phi(\underline{u}) \cdot \phi(\underline{v})\]
 
 Queste funzioni kernel sono delle funzioni di similarità calcolate nello spazio attributi originale di $x$ e sono riferite alla funzione kernel.

\subsubsection{Multi-Layer Perceptron o Artificial Neural Network}
Il Multi-Layer Perceptron (MLP) è una tecnica di classificazione che si basa sulla separazione dello spazio degli attributi; queste tecniche sono oggi molto utilizzate per il deep learning.
\begin{defn}
 	Si definisce \textbf{MLP Multi-Layer Perceptron} un modello che consiste in neuroni artificiali che comunicano in modo unidirezionale, dall'input X alla variabile di classe (volendo ci possono essere più neuroni di ouput).
\end{defn}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8 \linewidth]{classification/pict/mlp.png}
	\caption{Esmpio di MLP}
\end{figure}

In figura vi sono 3 neuroni di parametri continui a cui viene aggiunto un quarto neurone che calcola una funzione. Ad ogni input nel neurone è associato un peso $w$.
Il neurone calcola una \textbf{combinazione lineare} tra gli input ed il peso dato ad esso, il j-esimo neurone calcola: 

\[ y_j = f(\sum_{i=1}^{n} w_{i,j} \cdot x_i - \theta_j)\] 

Ad esempio il quarto neurone in figura vale:  
\[z_4 = w_{1,4} \cdot x_1 + w_{2,4} \cdot x_2 +w_{3,4} \cdot x_3\]

A questo calcolo viene posto una valore di soglia pari a $\theta$ applicato alla combinazione lineare. Successivamente,  il neurone risponde con un valore pari all'applicazione di una funzione di attivazione rispetto all'input meno il threshold, nel nostro caso $f(z_4-\theta_4)$. 

\begin{defn}
	Si definisce \textbf{funzione di attivazione} la funzione applicata ad un neurone che restituisce il valore che verrà portato ad un altro strato con cui questo neurone comunica.
\end{defn}
Storicamente le funzioni applicazione sono la tangente iperbolica e la funzione logistica (intervallo tra -1 e 1 e tra 0 e 1). Oggi vengono utilizzate delle funzioni non derivabili più complesse, come funzioni \textbf{RELU} (Retify Linear Unit) che assumono valore 0 nel semiasse negativo e 1 nel semiasse positivo.
\\

Vi sono 3 tipi di neuroni: di input, di output e nascosti:
\begin{itemize}
	\item \textbf{Input} sono collegati con le variabili esplicative e con \textit{ciascun} nodo nascosto.
	\item \textbf{Nascosti} (o hidden) ricevono i segnali dai neuroni di input e il segnale viene propagato a quelli di ouput.
	\item \textbf{Output} sono associati alla variabile di classe e ricevono i segnali da visualizzare.
\end{itemize}

Ogni neurone di input è connesso con ogni neuroni di strato nascosto  ed ogni nodo di strato nascosto comunica con il nodo di output, rete \textbf{fully-connected}. Ogni arco ha associato un peso e ogni nodo ha un valore di soglia $\theta_j$ e una funzione di attivazione. 

\begin{figure}[H]
	\centering
	\includegraphics[height=0.5 \linewidth]{classification/pict/mlp_struct.png}
	\caption{Modello MLP semplice}
\end{figure}

Per la disposizione dell'architettura ho diverse scelte da fare: quanti neuroni usare, quanti nello strato nascosto, quanti strati nascosti usiamo, che funzione di attivazione, ecc.

Determinare l'architettura della nostra rete è fondamentale per avere buone performance. Posso pensare di aggiungere un \textbf{livello} di neuroni nascosti. Non vi sono vincoli rispetto a comunicare saltando livelli. \textbf{Non} si può però comunicare all'indietro, infatti queste reti sono chiamate \textbf{feed-forward neural network} (possono possedere fino a centinaia di strati nascosti).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9 \linewidth]{classification/pict/mlp_esempio.png}
	\caption{Esempio MPL con 2 strati nascosti}
\end{figure}

Per migliorare l'apprendimento si possono propagare le valutazioni fatte all'output per tutti i nodi risalendo fino all'input modificando i pesi degli archi. Questa cosa funziona bene e ha senso con le RELU.

Il problema MLP. non ha soluzione oggi, pertanto si procede in modo \textbf{empirico}, non è ancora possibile, infatti, arrivare ad una soluzione ottima, non si riesce a capire se si ha raggiunto il massimo/minimo globale ma si cerca quella che dà risultati accettabili e migliori di altri. Il numero di nodi, archi o livelli nascosti è quindi scelto in base all'esperienza.


\subsubsection{Classificatori bayesiani}
I \textbf{classificatori probabilistici} calcolano la probabilità condizionata usando il \textbf{teorema di Bayes} e cercano di capire il valore della classe attributo dalle altre variabili. 

\begin{defn}

La \textbf{formula di Bayes} calcola la probabilità che un evento accada dati altri eventi: 

\[P(Y|\underline{X}) = \frac{P(\underline{X}|Y) \cdot P(Y)}{P(\underline{X})}\]

Dove:
\begin{itemize}
	\item $P(Y)$ è la probabilità dell' attributo di classe.
	\item $P(\underline{X}|Y)$ la verosimiglianza di un vettore di attributi dato l'attributo di classe.
	\item $P(\underline{X})$ probabilità dell'evidenza (nel senso di certezza).
	\item $P(Y|\underline{X})$ probabilità a  posteriori dell'attributo di classe  dato il vettore di attributi esplicativi.
\end{itemize}

Vediamone un esempio, assumiamo di avere:
\begin{itemize}
	\item $Y$ variabile binaria di classe $\{-1,+1\}$
	\item $\underline{X}$ variabile bianria esplicativa $\{Male, Female\}$
\end{itemize}
\end{defn}
Assumiamo di avere le seguenti probabilità: 
\[P(Y) = (0.3,0.7) \quad P(\underline{X}|Y=-1) = (0.2,0.8) \quad P(\underline{X}|Y = +1) = (0.9,0.1)\]

Vogliamo classificare $\underline{X} = Male$ tramite la formula di Bayes:

\[P(Y = -1 | \underline{X} = M) = \frac{P(\underline{X} = M | Y=-1) \cdot P(Y=-1)}{P(\underline{X}= M)} = \frac{0.06}{P(\underline{X} = M)}\]

\[P(Y = +1 | \underline{X} = M) = \frac{P(\underline{X} = M | Y=+1) \cdot P(Y=+1)}{P(\underline{X}= M)} = \frac{0.63}{P(\underline{X} = M)}\]

In questo caso quindi la $Y$ è più probabile che abbia valore $+1$ se $\underline{X} = Male$.

Assumiamo ora che il numero di variabili esplicative aumenti all'aumentare \textbf{n} ed esse sono binarie, così come l'attributo di classe, allora avremo bisogno di conoscere i seguenti parametri:

\[\theta_{ki} = P(\underline{X} = x_k | Y = y_i) \qquad k \in \{1, ..., 2^n\}, y_i \in \{-1,+1\}\]

Quando il numero di variabili esplicative cresce, le devo binarizzare quindi avrò $2^n$ parametri:

\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		\textbf{n} & 1 & 2 & 3 & 10 & 20 & 30 \\
		\hline
		\textbf{$\#$ parametri}& $2$ & $4$ & $8$ & $1,024$ & $1,048,576$ & $1,073,741,824$  \\
		\hline
	\end{tabular}
\end{table}

Per risolvere il problema si usa l'\textbf{assunto della condizione di indipendenza}.

\subsubsection{Naive bayes}

\begin{defn}
Dati gli attributi X, Y, Z, diremo che X è \textbf{indipendente condizionatamente} da Y dato Z, se e solo se la probabilità di X è indipendente dal valore dell'attributo Y una volta che Z sia noto, formalmente: 

\[\forall i,j,k P(X=x_i|Y=y_j, Z=z_k) = P(X=x_i|Z=z_k)\]
\end{defn}

Assumendo questa espressione per le variabili utilizzate si riduce enormemente il numero di parametri da calcolare; il \textbf{Naive Bayes} assume indipendenza condizionale e ci permette di calcolare la probabilità a posteriori dell'attributo classe dati gli attributi esplicativi come segue: 

\[P(X_1, ..., X_n|Y) = \prod_{i=1}^n P(X_i|Y)\]

\[2 \cdot (2^n-1) \rightarrow 2 \cdot n\]

Il \textbf{Classificatore Naive Bayes} calcola la probabilità a posteriori dell'attributo di classe nel modo seguente:

\[P(Y=y_k|X_1,...,X_n) = \frac{P(Y=y_k)\cdot \prod_{i=1}^{n}P(X_i|Y=y_k)}{\sum_{j}P(Y=y_j) \cdot \prod_{i=1}^{n}P(X_i|Y=y_j)} \]

Il record di variabili viene etichettato con il valore di classe che massimizza la probabilità a posteriori

\[arg\max_{y_k} P(Y = y_k | X_1,...,X_n)\]

Il modello Naive Bayes fa parte dei modelli \textbf{grafico-probabilistici}. Esiste un' intera famiglia di modelli di questo tipo: reti bayesiane dinamiche, ecc... non andremo nel dettaglio.

\begin{figure}[h!]
	\centering
	\includegraphics[height=0.3 \linewidth]{classification/pict/naivebayes.png}
	\caption{Rete naive bayes}
\end{figure}

A fianco del risultato output comunque è bene fornire la probabilità con il quale si è ottenuto il risultato, in modo da dare una \textit{misura di affidabilità.} 

Il classificatore Naive bayes può essere applicato ad attributi categorici (nominali e ordinali), numerici (intervalli e tassi). Ad ogni attributo numerico è associata una densità di probabilità condizionale di classe normale:

\[P(X_i = x_i | Y = y_k) = \frac{1}{\sigma_{ik}\sqrt{2 \cdot \pi}}exp(-\frac{(x_i-\mu_{ik})^2}{2 \sigma_{ik}^2})\] 

i cui parametri sono: 
\[\mu_{ik} = E[X_i | Y = y_k] \qquad \sigma_{ik}^2 = E[(X_i - \mu_{ik})^2 | Y = y_k]\]

\textbf{NB:} solitamente si combinano attributi categorici (nominali e ordinali) con attributi numerici (intervalli e tassi).

Il modello Naive bayes si comporta generalmente bene ma richiede \textit{una enorme premessa} per essere applicato correttamente, ovvero l' (\textbf{indipendenza condizionata}). Per ovviare a questo problema è stata creata una versione più flessibile mantenendo la stessa capacità computazionale.

\subsubsection{Reti bayesiane}
Il classificatore Naive bayes viene generalizzato dal modello di rete \textbf{Rete bayesiana} che è meno forzato dall'assunzione di indipendenza condizionata ma sfrutta il concetto di \textbf{Sparsity}. 

Un esempio di rete bayesiana è il seguente:

\begin{figure}[H]
	\centering
	\includegraphics[height=0.35 \linewidth]{classification/pict/networkbayes.png}
	\caption{Esempio di rete bayesiana generalizzata}
\end{figure}
Gli attributi esplicativi sono associati tramite i nodi, ma non solo quelli con un arco in entrata dalla radice; gli attributi esplicativi possono puntare ad altri attributi esplicativi.
Nel grafo \textbf{non} vi possono essere \textbf{cicli} in quanto un nodo successivo non può essere causa di un nodo precedente. Per ogni nodo bisogna specificare una tabella di probabilità condizionata rispetto al valore dei suoi genitori. In sostanza prendo in considerazione tutte le possibili configurazioni dei genitori e per ognuna do un output diverso. 

In questo modello gli attributi esplicativi non sono più assunti come indipententi dalla classe attributo.

Questo è un modello particolare di rete bayesiana che viene utilizzato per la classificazione. La cosa bella di questo modello è che anche con valori null si può comunque fare inferenza perché nativamente ha questa caratteristica. 

\textit{Nelle reti neurali non è possibile far computare un modello senza avere tutti gli input. }

\subsubsection{Tree-augmented Naive Bayes}
Vi sono diverse versioni di reti bayesiane, una di queste è il Tree-augmented Naive Bayes.
\begin{defn}
Si definisce \textbf{Tree-augmented Naive Bayes} una rete che oltre ad avere il nodo genitore Y può permettersi di avere un altro nodo genitore (sempre). 
\end{defn}

Se io addestro la rete senza nodo di classe allora ho un albero, dopo inserendovi il nodo Y fa inferenza (nel caso in figura vedi nodo $X_1$).

\begin{figure}[H]
	\centering
	\includegraphics[height=0.35 \linewidth]{classification/pict/treenaivebayes.png}
	\caption{tree-augmented Naive bayes}
\end{figure}

\`E molto potente per la feature selection, ovvero per la ricerca delle feature più significative. 
Ci sono altre modifiche possibili al Tree-augmente naive bayes, presenti il letteratura, come AODE, HNB, ecc... 

\subsubsection{Summary}
Per capire come \textit{nativamente} si differenziano i modelli di classificazione è presente la  tabella successiva. 

\begin{figure}[H]
	\centering
	\includegraphics[height=0.3 \linewidth]{classification/pict/class_tecniques.png}
	\caption{Comparazione tra i principali modelli nativi}
\end{figure}

In ogni caso, è possibile \textbf{trasformare} attributi esplicativi e di classe nominali applicando tecniche di classificazione non assegnate per attributi nominali. \`E da notare comunque che le tecniche di classificazione \textbf{non} sono l'opzione migliore quando bisogna predire un attributo \textbf{ordinale}.

\subsection{Performance Evaluation (*)}
La stima dell'accuratezza non è sufficiente da sola per comprendere la confidenza di un modello, in particolare, quando bisogna applicarlo a dati mai visti.
Bisogna rendersi conto del rischio di \textbf{overfitting} quindi magnificare alcuni risultati, e di \textbf{underfitting} ovvero non abbastanza accurati.

Vi sono due diversi tipi di \textit{errori}:
\begin{itemize}
	\item \textbf{Training error}: numero di record del training set mal classificato.
	\item \textbf{Generalization error}: errori su record no visti precedentemente (test set).
\end{itemize}
\textit{Un buon classificatore non deve fittare troppo bene il training set ma deve anche classificare accuratamente i record che non ha mai visto prima.}

\begin{defn}
	Si parla di \textbf{Overfitting del modello} quando un modello di classificazione ottiene delle performance molto alte sul training set ma ha un generalization error molto alto.
\end{defn}

\underline{Nella pratica}: non bisogna adattare troppo il modello sui dati di training ma bisogna puntare ad un buon compromesso tra l'errore fatto sul training set e quello fatto sul test set. 

Consideriamo il seguente esempio:
\begin{figure}[H]
	\centering
	\includegraphics[height=0.3 \linewidth]{classification/pict/overfitting.png}
	\caption{Esempio di overfitting}
\end{figure}
\begin{itemize}
	\item Linea {\color{blue}{blu}}: modello con poco training error (non overfitting)
	\item Linea {\color{orange}{arancione}}: modello con zero training error (overfitting)
\end{itemize}
Come si può notare il modello {\color{orange}{arancione}} si adatta troppo ai dati di training quindi c'è un forte rischio che non classifichi bene i dati di test. Al contrario il modello {\color{blue}{blu}} segue meglio la tendenza della distribuzione delle due classi di elementi.

Ora vediamo come si comportano i due modelli nel test set
\begin{figure}[H]
	\centering
	\includegraphics[height=0.3 \linewidth]{classification/pict/overfitting_test.png}
	\caption{Esempio di overfitting test}
\end{figure}
Ora si può notare come il modello {\color{blue}{blu}}: più alto training error e più basso generalization error) classifichi \textbf{più correttamente} rispetto al modello {\color{orange}{arancione}}: più basso training error e più alto generalization error).

\begin{defn}
Il fenomeno contrario è definito \textbf{Underfitting}, in cui sia il training error che il generation error sono elevati. Accade quando performance del training e test sono simili e basse.
\end{defn}
La complessità del modello scelto non è sufficiente per i dati che sto studiando. Non abbiamo utilizzato tutta la flessibilità che potevamo usare nella definizione del modello.

\begin{figure}[H]
	\centering
	\includegraphics[height=0.3 \linewidth]{classification/pict/underfitting_merge.png}
	\caption{Esempio di underfitting (sinistra)}
\end{figure}
Guardando agli esempi in figura, il modello {\color{blue}{blu}} classifica erroneamente 6 record, per questo non è un buon modello. Invece, il modello {\color{orange}{arancione}} fitta meglio il training ed risponde meglio anche al test (1 errore), in questo caso però bisogna stare attenti a non incappare in overfitting.
 
La \textbf{soluzione ottimale} la si evince tenendo conto di quello che succede sia nei dati di training che nei dati di test.

\textit{\`E impossibile avere una misurazione certa sugli errori del modello, si può solo avere una stima. }

\subsubsection{Performance di un modello di classificazione}

In una analisi di classificazione l'obiettivo è sviluppare un modello che dia una migliore classificazione possibile. Viene valutata in termini di:
\begin{itemize}
	\item \textit{Accuracy}
	\item \textit{Speed}
	\item \textit{Robustezza}
	\item \textit{Scalabilità}
	\item \textit{Interpretabilità} (tema più dibattuto)
\end{itemize}

\subsubsection{Accuratezza}
L'accuratezza è una misura fondamentale perché: 
\begin{itemize}
	\item Misura la capacità del modello nel dare classificazioni affidabili su nuovi record
	\item Permette di selezionare l'istanza di modello che fornisce le migliori performance sui nuovi record
\end{itemize}
Consideriamo:

$D_T$ training set con $t$ record.

$D_{TS}$ test set con $v$ record.

$D = D_T \cup D_{TS},D_T \cap D_{TS} = \emptyset,m = t + v$\\
\textit{Un buon indicatore è la percentuale di record di test ($D_{TS}$) che sono classificati correttamente. }

Definiamo:
\begin{itemize}
	\item $y_i$ come il valore della variabile di classe dell'istanza $\underline{x_i} \in D_{TS}$
	\item $f(\underline{x_i})$ valore della classe predetto per l'istanza $\underline{x_i} \in D_{TS}$ dal modello di classificazione
\end{itemize} 
Consideriamo quindi la seguente funzione di Loss:
\[
L(y_i, f(\underline{x_i})) =  
\begin{cases}
	0 \qquad se &y_i = f(\underline{x_i}) \\
	1 \qquad se &y_i \ne f(\underline{x_i})
\end{cases}
\]
L'\textbf{Accuratezza} viene calcolata in questo modo:
\[ acc(D_{TS}) = 1 - \frac{1}{v} \sum_{i=1}^{v} L(y_i, f(\underline{x_i}))\]
In alcuni casi si preferisce l'\textbf{Errore} associato:
\[err(D_{TS}) = 1 - acc(D_{TS}) = \frac{1}{v} \sum_{i=1}^{v} L(y_i, f(\underline{x_i}))\]

\subsubsection{Speed}
Gli algoritmi di classificazione differiscono per:
\begin{itemize}
	\item Tempo di apprendimento
	\item Spazio di memoria occupato
\end{itemize}
Oggi però si dà meno importanza a queste caratteristiche in quanto vi sono diverse tecnologie e tecniche per ottimizzare i tempi e lo spazio di memoria costa sempre meno (economicamente parlando).

Un algoritmo che impiega molto tempo e/o una grande quantità di memoria per l'addestramento, può essere trainato dopo un campionamento del dataset originale. In questi casi accettiamo di non sfruttare tutte le informazioni disponibili invece di trainare un tipo di modello che riteniamo assicurerà buone performance.

\subsubsection{Robustezza}
\begin{defn}
	Un modello/algoritmo può essere \textbf{robusto} o meno rispetto a:
\begin{itemize}
	\item \textit{Outliers}: possono influenzare significativamente il modello
	\item \textit{Misssing data}: problema centrale (i dati di base sono sporchi e soggetti a deperimento)
	\item \textit{Variazione tra training set e test set}: si parte dal presupposto che i dati di training siano simili e utili per quelli di test, se questa affermazione decade non \`e pi\`u robusto l'algoritmo
\end{itemize}
\subsubsection{Scalabilità}
\end{defn}

\begin{defn}
	Si definisce \textbf{scalabilità} la capacità di apprendere da enormi quantità di dati. Questa proprietà è intresecamente connessa alla speed. 
\end{defn}
Es. le reti bayesiane scalano molto male. 


\subsubsection{Interpretabilità}
\begin{defn}
Quando la classificazione è orientata dall'\textbf{Interpretabilità} di un problema per essere risolto  non ci si può limitare ad assicurare alti valori di accuratezza, è importante estrarre regole semplici e chiare per l'esperto di dominio del caso di studio.
\end{defn}
L'interpretabilità è un problema enorme, perché noi umani non ragioniamo in termini quantitativi, ma qualitativi. Una spiegazione può essere la più dettagliata e precisa possibile ma se non intercetta il linguaggio e il contesto in cui vive il soggetto che voglio raggiungere, non viene raggiunto l'obiettivo di far interpretare il lavoro.

\subsubsection{Holdout}
\begin{defn}
	Si definisce \textbf{Holdout} la partizione del dataset $D$ in due sottinsiemi di training e test set attraverso un procedura di campionamento. 
\end{defn}

\textbf{Best practice} suggerisce 2/3 per il training set e 1/3 per il test set.

Se si hanno molti dati a disposizione non \`e necessario seguire questa proporzione si pu\`o aumentare sul training.\\

\begin{figure}[H]
	\centering
	\includegraphics[height=0.3 \linewidth]{classification/pict/holdout.png}
	\caption{procedura di Holdout}
\end{figure}
\textit{La stima dell'accuratezza dipende fortemente dalla particolare scelta di divisione tra training e test set.} Vi sono alcune tecniche per rendere più efficiente la divisione.

\begin{defn}
	Si definisce\textbf{Iterated Holdout} la tecnica che consiste nel ripetere iterativamente $r$ volte il metodo di holdout per cui apprendo e stimo l'accuratezza. 
\end{defn}
Ad ogni iterazione $r$ si estrae un campione random $D_{Tr}$ di $t$ record,  ottenendo: 
	
\[D_{TS_r} = D - D_{T_r}\]
	
Si ripete la procedura $r$ volte e l'accuratezza del classificatore è stimanata dalla media dei valori di accuratezza campionati $acc(D_{TS_r})$ calcolati su ogni test set $D_{TS_r}$:
	
\[ acc = \frac{1}{R} \sum_{r=1}^{R}acc(D_{TS_r})\]\\
	
Il numero di iterazioni $r$ può essere scelto tramite specifiche tecniche statistiche.

\textit{L'iterated Holdout è chiaramente un metodo più robusto, le sue performance hanno un bias più piccolo rispetto al normale holdout; tuttavia, non permette il controllo del numero di volte in cui un record è presente nel training e test set.} Non si ha la certezza di aver usato dei dati ottimi per la stima fatta. In alcuni casi si ricorre a uno schema più efficace, capace di ridurre l'impatto degli outlier. 

\begin{defn}
	Si definisce \textbf{Cross-validation} la tecnica di holdout che garantisce che ciascun record di un dataset $D$ sia incluso nel training set con lo stesso numero di volte ed esattamente una volta nel test set. 
\end{defn}
\textit{Il dataset $D$ viene partizionato in $K$ sottoinsiemi disgiunti detti \textbf{fold}, esaustivi e con circa lo stesso numero di record: }

\[D_1, D_2, ..., D_K\]

Eseguiamo $K$ iterazioni training-test. Alla k-esima iterazione avremo:

\[D_{T_k} = \{D_1, ...,D_{k-1},D_{k+1}, ...,D_K\} \qquad D_{TS_k} = D_k\]

L'insieme $D_{T_k}$ è usato per il training  mentre il $D_K$ viene usato per il test della k-esima iterazione, in pratica ho più training set e un test set.; l'accuratezza viene, quindi, stimata facendo la media delle $K$ iterazioni:

\[acc = \frac{1}{K} \sum_{k=1}^K acc(D_k)\]
Esistono diverse selezioni per il valore di $K$. I valori tipici sono $K = 3,5,10$ mentre un caso limite viene applicato quando i dati sono scarsi chiamato \textbf{Leave One Out Cross Validation} (\textbf{LOOCV}). \textit{LOOCV è una tecnica che assume che ogni record sia una partizione del dataset, così il valore di $K$ equivale al numero di record del dataset $D$.}

\begin{figure}[H]
	\centering
	\includegraphics[height=0.4 \linewidth]{classification/pict/cross_validation.png}
	\caption{Procedura di cross validation}
\end{figure}

Solitamente ci si chiede se ogni partizione del dataset contenga la \textit{stessa proporzione} di valori possibili della classe attributo. Quando le proporzioni sono fortemente sbilanciate si adottano specifiche tecniche di campionamento,es. \textbf{campionamento stratificato}. 
\`E buona norma fidarsi di più del \textbf{k-folds cross-validation} in quanto rispetto all'\textit{iterated holdout} si ha la certezza che i record non siano ripetuti.

\subsection{Comparing Classifiers (*)}
La comparazione tra due diversi \textit{classificatori} non è semplice, perché dipende dai differenti modelli utilizzati e dalla distribuzione dei dati, nonché dalla loro quantità. L'accuratezza ha diverso valore se ricavata da pochi record di dati. 
es. 
\begin{itemize}
	\item \textbf{Inducer A}: accuracy = 0.85 su un test set di 30 record
	\item \textbf{Inducer B}: accuracy = 0.75 su un test set di 5 000 record
\end{itemize}
Qual è il migliore?

\textit{L'accuratezza da sola non basta. Bisogna stimare l'intervallo di confidenza dell'accuratezza per i due inducer e testare l'importanza statistica delle deviazioni osservate.}
 
\subsubsection{Intervallo di confidenza}
Consideriamo il problema di predire il valore di una classe attributo da un test record come esperimento \textbf{binomiale}.

Dato un test set $D_N$ contenente $N$ record, consideriamo:
\begin{itemize}
	\item \textbf{X}: numero di record correttamente predetti dall'inducer
	\item \textbf{p}: vera, ma non la conosciuamo, accuratezza dell'inducer (probabilità di successo della distribuzione binomiale)
\end{itemize}
Modelliamo il problema con X distribuita secondo una binomiale con
\begin{itemize}
	\item media = $N \cdot p$
	\item varianza = $N \cdot p \cdot (1-p)$
\end{itemize}
L'oggetto del nostro studio è l'\textbf{accuratezza empirica} calcolata come:
 \[acc = \frac{X}{N} \] 
Che è distribuita secondo una binomiale con $\mu = p$ e $\sigma^2 = \frac{p \cdot (1-p)}{N}$

Sebbene la distribuzione binomiale possa essere usata per stimare l'intervallo di confidenza per l'accuratezza, se la dimensione del test set è sufficientemente grande, è buona norma approssimare la distribuzione con una \textbf{normale}. Così facendo l'intervallo di confidenza per l'\textbf{accuratezza empirica} diventa:

\[ P \Bigl( -Z_{1-\alpha/2} < \frac{acc - p}{\sqrt{p \cdot (1-p / N)}} < Z_{1 - \alpha/2} \Bigr) = 1 - \alpha \]\\

Rimodellando, la disuguaglianza ci porta al seguente intervallo di confidenza con confidenza $1-\alpha$ per \textbf{p}:

\[ \Biggl[ \frac{acc + \frac{Z_{1-\frac{a}{2}}^2 }{2 \cdot N} - Z_{1-\frac{a}{2}} \cdot \sqrt{\frac{acc}{N} - \frac{acc^2}{N} + \frac{Z_{1-\frac{a}{2}}^2}{4 \cdot N^2}}}{\bigl(1 + \frac{Z_{1-\frac{a}{2}^2}}{N}\bigr)}
, 
\frac{acc + \frac{Z_{1-\frac{a}{2}}^2 }{2 \cdot N} + Z_{1-\frac{a}{2}} \cdot \sqrt{\frac{acc}{N} - \frac{acc^2}{N} + \frac{Z_{1-\frac{a}{2}}^2}{4 \cdot N^2}}}{\bigl(1 + \frac{Z_{1-\frac{a}{2}^2}}{N}\bigr)} \Biggr] \]
\textbf{NB}: se prendessi diversi test set indipendenti l'uno dall'altro è ovvio che vengono diversi intervalli di confidenza. Però in questi casi fissato $\alpha$ posso stabilire statisticamente quali intervalli siano più significativi di altri. Più si  riduce $\alpha$ più aumenta $\beta$ ovvero il caso di errore in cui non viene rifiutata l'ipotesi nulla quando invece dovrei rifiutarla.

Dato un modello con acc=0.8 sul test set di $100$ record, fissiamo l'intervallo di confidenza per l'accuratezza a $0.95\%$. L'andamento dell'intervallo è il seguente:

\begin{figure}[H]
	\centering
	\includegraphics[height=0.5 \linewidth]{classification/pict/inter_confidence.png}
	\caption{Andamento dell'intervallo di confidenza}
\end{figure}

\subsubsection{Different test set}
Supponiamo il caso in cui sono presenti 2 modelli e vengono valutati su due test set diversi (è una cosa molto comune e che può accadere per numerevoli ragioni):
\begin{itemize}
	\item $M_1$ valutato su test set $D_1$ contenente $n_1$ record con tasso di errore $e_1$
	\item $M_2$ valutato su test set $D_2$ contenente $n_2$ record con tasso di errore $e_2$
\end{itemize}
I due test set devono essere assunti \textit{indipendenti}. Il nostro obiettivo è testare se la \textbf{differenza} tra i due errori è statisticamente significativa.
Assumendo che $n_1$ e $n_2$ siano sufficientemente grandi, allora i tassi di errore $e_1$ ed $e_2$ possono essere approssimati usando distribuzioni \textit{normali}. \\
La differenza osservata è denotata come: 
\[d = e_1 - e_2\]
Sono distribuiti secondo una \textbf{normale} con 
\[\mu = d_t \qquad \sigma^2 = \sigma^2_d\] 
La varianza di $d$ può essere stimata come segue: 
\[\sigma^2 \cong \hat{\sigma}_d^2 = \frac{e_1 \cdot (1-e_1)}{n_1} + \frac{e_2 \cdot (1-e_2)}{n_2}\]
L'intervallo di confidenza per la differenza $d_t$ è: 

\[ \bigl( d - z_{1-\alpha/2} \cdot \hat{\sigma}_d, d + z_{1-\alpha/2} \cdot \hat{\sigma}_d \bigr) \]
Ci possono essere sostanzialmente 3 casi:
\begin{enumerate}
	\item \[0 \in \bigl( d - z_{1-\alpha/2} \cdot \hat{\sigma}_d, d + z_{1-\alpha/2} \cdot \hat{\sigma}_d \bigr)\] 
	In questo caso si conclude che la differenza osservata \textbf{non è statisticamente significativa} ad un livello $\alpha$, i modelli non sono significativamente differenti 
	\item \[d + z_{1-\alpha/2} \cdot \hat{\sigma}_d < 0\] 
	Se il limite superiore della confidenza è negativo allora il modello \textbf{$M_1$ è migliore del modello $M_2$} a un livello $\alpha/2$
	\item \[d - z_{1-\alpha/2} \cdot \hat{\sigma}_d > 0\] 
	Se il limite inferiore della confidenza è positivo allora il modello \textbf{$M_2$ è migliore del modello $M_1$} a un livello $\alpha/2$
\end{enumerate}

\`E buona norma fare il test di ipotesi \textbf{solo} se prima mi faccio la domanda se siano o meno differenti, non bisogna  condurre test a casaccio o a tappeto perché altrimenti si rischia di raggiungere dei risultati assurdi. Bisogna porsi la domanda e poi eseguire tutta la procedura sulla coppia di modelli, in quanto vi è un \textbf{problema dei confronti multipli} devo adattarli tutti allo stesso livello di $\alpha$ altrimenti le differenze non sono confrontabili.

\subsubsection{Same test set}
Nel caso in cui sia possibile utilizzare lo \textbf{stesso test set} si può svolgere un test più \textbf{potente}; ovvero, si può valutare il caso di errore di secondo tipo: si afferma che la differenza tra i due classificatori non è significativa quando in realtà lo è ($\beta$). 

Si confrontano $M_1$ ed $M_2$ usando il \textit{k-fold cross validation}.
Ho il dataset $D$ partizionato in $K$ subset disgiunti con circa lo stesso numero di record. 
\[D_1, D_2, ..., D_K\]
Si applicano le tecniche di classificazione per costruire i modelli $M_1$ e $M_2$ dalle $k-1$ partizioni e si testano con la partizione rimanente. Il passaggio deve essere ripetuto $K$ volte, ogni volta usando una differente partizione per il test set. Si ottiene:
\begin{itemize}
	\item $M_{1k}$ \textbf{inducer} per il modello $M_1$ ottenuto alla k-esima iterazione con $e_{1k}$ errore
	\item $M_{2k}$ \textbf{inducer} per il modello $M_2$ ottenuto alla k-esima iterazione con $e_{2k}$ errore
\end{itemize}
La differenza tra gli errori durante la k-esima iterazione è:
\[d_k = e_{1k} - e_{2k}\]
Per K sufficientemente grande, allora $d_k$ è distribuito \textit{normalmente} con: 
\[\mu = d_t^{cv} \quad \sigma = \sigma^{cv}\]
Dove la varianza osservata è stimata utilizzando la seguente formula: 

\[ \hat{\sigma}^2_{d^{cv}} = \frac{\sum_{k=1}^{K}(d_k - \bar{d})^2}{K \cdot (K-1)} \qquad \bar{d} = \frac{1}{K} \sum_{k=1}^K d_k \]
Usiamo una distribuzione T di Student per calcolare l'intervallo di confidenza per il valore della vera media $d_t^{cv}$

\[\biggl( \hat{d} - t_{1-\frac{\alpha}{2}}^{K-1} \cdot \hat{\sigma}_{d^{cv}}
,
\hat{d} + t_{1-\frac{\alpha}{2}}^{K-1} \cdot \hat{\sigma}_{d^{cv}} \biggr)\]
Dove 
\[t_{1-\frac{\alpha}{2}}^{K-1}\]
\`E ottenuta dalla tavola di probabilità, il quantile associato alla confidenza $1-\alpha$ e $K-1$ gradi di libertà.\\
Ovviamente valgono \textbf{le stesse considerazioni} fatte precedentemente sull'intervallo di confidenza per la differenza tra due classificatori: se l'intervallo di confidenza contiene il valore 0, concludiamo che la differenza osservata \textit{non è statisticamente significativa} al livello $\alpha$ (confidenza $1-\alpha$). 

\subsection{Class Imbalance Problem (*)}
Consideriamo di nuovo il dataset dei Churner. In particolare è noto che il $14.5\%$ sono dei churner. Un modello di classificazione che etichetta i record come non churner avrà per forza un'alta accuracy in quanto è la classe più frequente nel dataset. 

\begin{defn}
	Si definisce la \textbf{ZeroR Rule}	il modello che risponde sempre con la classe più frequente, risultando, quindi, il modello più inutile.	
\end{defn}
La misura di \textit{accuracy} tratta classi equamente importanti, non è adatta ad analizzzare \textbf{dataset sbilanciati},  dove la classe più rara è considerata \textit{più interessante} di quella più frequente. Per la classificazione binaria si distingue:
\begin{itemize}
	\item La \textbf{classe positiva} ovvero la classe più rara (o meno frequente)
	\item La \textbf{classe negativa} ovvero la classe più frequente
\end{itemize}
Consideriamo la \textbf{matrice di confusione}.
\begin{figure}[H]
	\centering
	\includegraphics[height=0.25 \linewidth]{classification/pict/matrconf.png}
	\caption{Matrice di confusione binaria}
\end{figure}
Dalla matrice calcoliamo i seguenti indicatori:
\begin{itemize}
	\item 		\textbf{TNR, specificità} (\textit{tasso veri negativi}):  frazione di record negativi correttamente predetti dal modello  \[TNR = \frac{TN}{TN + FP}\]
	\item	\textbf{TPR, sensitività} (\textit{tasso veri positivi}): frazione dei record positivi correttamente predetti dal modello \[TPR = \frac{TP}{TP + FN}\]
	\item \textbf{FPR, tasso falsi positivi}: frazione dei record negativi predetti come classe positiva dal modello \[FPR = \frac{FP}{TN + FP}\]
	\item \textbf{FNR, tasso falsi negativi}: frazione dei record positivi predetti come classe negativa dal modello \[TNR = \frac{FN}{TP + FN}\]
\end{itemize}
Le metriche che si calcolano nella ricerca di una classe più importante delle altre sono la \textbf{Precision} e il \textbf{Recall}.
\begin{defn}
	La \textbf{Precision} determina la frazione di record che effettivamente si rivela essere positiva nel gruppo che il classificatore definisce come classe positiva. 
	\[p = \frac{TP}{TP+FP}\]
\end{defn}
Si prende il punto di vista del classificatore. Valuto quanti di quelli positivi ha valutato come effettivamente positivi. Più è \textit{alta}  la precisione più è \textit{basso} il numero di falsi positivi commessi.

\begin{defn}
	L'indice \textbf{Recall} misura la frazione di record positivi correttamente predetti dal modello di classificazione; in particolare, è definito come:
	\[r = \frac{TP}{TP+FN}\]
\end{defn}
In questo caso si prende il punto di vista della realtà. Al denominatore si hanno quelli effettivamente positivi, al numeratore ho quelli che il classificatore ritiene positivi. Un \textit{alto} Recall signfica \textit{pochi} record positivi \textit{erroneamente} classificati come classe negativa. Infatti, il Recall è equivalente al $TPR$.

Vediamo un esempio sul problema del churn:
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]
	{classification/pict/esPrecisionRecall_merge.png}
	\caption{Esempio calcolo precision e recall}
\end{figure}
Vi è un \textit{legame molto stretto tra questi due indici}, si può avere un recall molto alto ma allora probabilmente avrò una precision bassa. Oppure si può puntare ad avere una precisione alta ma in quei casi probabilmente si avrà una recall bassa.

\textit{Queste due misure per convenzione sono calcolate sulla classe positiva (quella meno frequente) non \`e per\`o detto che non vadano utilizzate su quella negativa. Inoltre \textit{non} \`e sempre detto che queste quantit\`a siano sempre calcolabili (possibile divisione $0/0$).}

Siccome queste due misure sono legate (pensa alla problema della coperta corta). Si utilizza una misura che le riassume.
\begin{defn}
	Si definisce \textbf{$F_1$ mesure}  la media armonica tra \textbf{Recall} e \textbf{Precision}. 
	\[ F_1 = \frac{2 \cdot r \cdot p}{r + p}\]
\end{defn}	
Per come è definita un alto valore di $F_1$ implica alti valori di recall e precision.
Esiste una generalizzazione sotto il nome di \textbf{$F_\beta$ mesure} per esaminare il tradeoff tra Precision e Recall:
	\[ F_\beta = \frac{(\beta^2 + 1) \cdot r \cdot p}{r + \beta^2 \cdot p} \]
\begin{itemize}
	\item $F_\beta$ con $\beta = 0$ è la Precision
	\item $F_\beta$ con $\beta = \infty$ è la Recall
\end{itemize}


\subsection{Counting the cost (*)}

\subsubsection{Matrice di costo}
Se pensiamo al nostro problema del churner, l'azienda ha interesse ad evitare quello che si prevede, ovvero che il cliente se ne vada, in sostanza si cerca di prevedere chi vuole andarsene e si cerca di evitare che questa previsione sia effettiva applicando politiche di dissuasione. L'azienda può spendere un tot budget per invertire questo fenomeno, bisogna fare il meglio per identificare i possibili churner e attuare \textbf{solo} su di loro le politiche di dissuasione (se lo facessi per tutti i clienti non avrebbe senso per quanto riguarda i costi).
\\Bisogna convincere il proprio interlocutore che il modello sviluppato sia migliore rispetto al modello che storicamente hanno utilizzato. La misura più utilizzata per questo è l'accuracy legata alla matrice di confusione. In aggiunta, però, bisogna associarci la \textbf{matrice di costo}. 

\begin{defn}
	La \textbf{Matrice di Costo} stabilisce in base a falsi positivi e negativi quanto costo (in soldi/lavoro) sostiene l'azienda per classificare un certo soggetto (es. come churner o non churner). 
\end{defn}
	Il costo è così calcolato:
\begin{figure}[H]
	\centering
	\includegraphics[height=0.55 \linewidth]{classification/pict/matr_costo.png}
	\caption{Legame tra matrice di confusione (sn) e matrice di costo (dx)}
\end{figure}
\[Cost = C_{--} \cdot TN + C_{-+} \cdot FP + C_{+-} \cdot FN + C_{++} \cdot TP\]
\textbf{NB} se matrice di costo è simmetrica allora il costo corrisponde all'accuratezza; per comprenderlo, vediamo il seguente esempio. Confrontiamo le performance di accuracy e costo tra lo \textit{Standard Mailout Procedure} (\textbf{SMP}), procedura già utilizzata per dissuadere i clienti, con il modello nuovo di classificazione \textbf{M} da noi costruito:

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{classification/pict/esConfMatr.png}
	\caption{Matrici di confusione: a sinistra SMP, a destra il modello M}
\end{figure}
Ora, però, bisogna associarci la \textbf{matrice di costo} per le rilevazioni e ne risulta:
\begin{figure}[H]
	\centering
	\includegraphics[height=0.2 \linewidth]{classification/pict/esMatrCosto.png}
	\caption{Matrice di costo churner}
\end{figure}
Come si può notar,e nonostante il nostro modello abbia una misura di \textbf{accuratezza} migliore, ha un \textbf{costo} (in termini di soldi) superiore rispetto al modello storicamente utilizzato dall'azienda. Pertanto, in questo caso, si preferirà la vecchia procedura SMP piuttosto che la nostra soluzione.
\`E importante notare però che ci sono situazioni nelle quali perdere un cliente è più grave che perderne un altro. Devo capire se sto utilizzando la matrice dei costi vera, oppure, se è solo rappresentativa. Se essa non è certa posso ragionare in un altro modo.

\subsubsection{Cumulative Gains} 
Supponiamo che ci sia una popolazione di 1.334 clienti di cui 500 sono possibili churners. Per esperienza accumulata sappiamo che il $15\%$ dei clienti è un churner, quindi:
\\$1334*0.15 = 200$ churner sul totale;
\\$500*0.15 = 75$ churner sui potenziali churners.
\\\\
Se si considerasse positiva la classe dei churner e negativa quella dei non churner e si provasse un campionamento casuale su di essi si avrebbe questo risultato:
\begin{figure}[H]
	\centering
	\includegraphics[height=0.1 \linewidth]{classification/pict/esChurnerRandom.png}
\end{figure}

Il modello di classificazione sviluppato, se applicato, identifica sui potenziali churner il $60\%$ di chi effettivamente se ne va: 

$200*0.6 = 120$ churners corretamente identificati.
\begin{figure}[H]
	\centering
	\includegraphics[height=0.1 \linewidth]{classification/pict/esChurnerModel.png}
\end{figure}
Pertanto possiamo affermare che il nostro modello sia molto meglio della \textit{zeroRule}, ovvero il campionamento casuale. 

\begin{defn}
	Si definisce \textbf{Lift Factor} il rapporto tra il modello $M$ rispetto al campionamento casuale:
	
	\[Lift = \frac{performace(M)}{performance(zeroRule)}\]
\end{defn}
Questo coefficiente è usato per calcolare l'incremento di performance tra due modelli.
Nel nostro caso: $0.6/0.375 = 1.6$\\

\textbf{NB}: si considera che il nostro classificatore è bravo ad identificare i casi semplici ma rischia di sbagliare di molto in quelli complessi. Man mano che viene forzato a rispondere lui tenderà a rispondere in modo sempre più random.

Si può ora sfruttare il \textit{Lift factor} per comprendere il livello di \textbf{profittabilità} una volta che i costi coinvolti sono noti. In questo caso bisogna avere un sottoinsieme di customers che hanno \textit{alta proporzione} di record positivi, \textit{maggiore} rispetto al dataset di partenza.

\begin{enumerate}
	\item \textbf{Ordinare} gli output con probabilità di riconoscimento corretto \textbf{prob(y)} in ordine decrescente
	\item \textbf{Estrarre} un primo sottoinsieme (partendo dall'alto) e calcolare il \textit{lift factor}, per forza di cose sarà alto
	\item \textbf{Considerare} allora un sottoinsieme più grande (sempre partendo dal primo valore) e calcolare il \textit{lift factor}, ci si aspetta un valore minore rispetto al precedente
	\item \textbf{Ripetere} il punto 3 finché si includono tutti i record
\end{enumerate} 

Nell'esempio in figura due passaggi del procedimento 
\begin{figure}[H]
	\centering
	\includegraphics[height=0.6 \linewidth]{classification/pict/liftFactor.png}
	\caption{Calcolo del \textbf{lift factor} primo sottoinsieme (sinistra) e secondo sottoinsieme (destra)}
\end{figure}

\begin{defn}
	I valori di Lift factor così calcolati vanno a formare la cosiddetta curva dei \textbf{Cumulative Gains}.
\end{defn}
\begin{figure}[H]
	\centering
	\includegraphics[height=0.6 \linewidth]{classification/pict/cumulative_gains.png}
	\caption{Cumulative gains evidenziando un punto di alto valore aggiunto}
\end{figure}

\textbf{NB}: La retta {\color{ao(english)}verde} calcola la cumulative gain per un modello che risponde in maniera \textit{casuale}, sull'asse x è la \textit{recall}, i positivi riconosciuti correttamente (ripetendo il test ad ogni step scelto)

\subsubsection{Lift Chart}
La curva \textit{Cumulative Gains} può essere mappata direttamente nella \textbf{Lift Chart}.

\begin{figure}[H]
	\centering
	\includegraphics[height=0.45 \linewidth]{classification/pict/lift_chart.png}
	\caption{Da cumulative gain a lift chart}
\end{figure}

Queste valutazioni permettono di comprendere \textit{quando} il classificatore perde efficacia. Naturalmente più il campione è piccolo meglio funziona, in quanto si usa record con \textbf{prob(y)} di riconoscimento corretto molto alta, bisogna comprendere però il punto di massima accuratezza in rapporto alla percentuale di istanze.

\subsubsection{Curva ROC}

\begin{defn}
	\`E chiamata \textbf{ROC} e sta per Receiver Operating Characteristic curve la tecnica grafica per la valutazione di modelli di classificazione.
\end{defn} 
Assomiglia molto alla cumulative gains ma sugli assi sono presenti:
\begin{itemize}
	\item \textbf{asse x:} la percentuale dei record falsi positivi sopportabili (\textbf{FPR})
	\item \textbf{asse y:} la percentuale dei record veri positivi (\textbf{TPR})
\end{itemize}
Entrambi i valori sono espressi in \textit{percentuale sul totale}. 
\begin{figure}[H]
	\centering
	\includegraphics[height=0.6 \linewidth]{classification/pict/roc.png}
	\caption{Curva ROC}
\end{figure}
\textit{Nell'esempio in figura vengono utilizzati particolari sottoinsiemi di dati, questa dipendenza può essere ridotta applicando la Cross-validation. La curva ROC rappresenta la performance di un classificatore senza guardare la distribuzione della classe o il costo dell'errore; serve, quindi, a confrontare diversi classificatori per cercare di comprendere dove un classificatore \`e pi\`u o meno efficace}; un esempio è il seguente:

\begin{figure}[H]
	\centering
	\includegraphics[height=0.6 \linewidth]{classification/pict/roc_confronto.png}
	\caption{Confronto ROC tra due modelli}
\end{figure}

Un modello è \textbf{preferibile} ad un altro se è disposto a sopportare un certo numero di falsi positivi.


\subsection{Feature Selection (*)}
Quasi sempre non ha senso utilizzare tutti gli attributi per generare il modello che si vuole sviluppare; Si procede, quindi,  con la selezione delle feature.
La \textbf{feature selection} è un compito importantissimo ed è fortemente legato alla risoluzione del problema. Analizzare le relazioni tra le variabili in modo \textit{preliminare} potrebbe aiutare di molto nella ricerca della soluzione migliore al problema. 
\begin{defn}
La \textbf{feature selection} è il processo che tenta di scoprire quali attributi sono:
\begin{itemize}
	\item \textit{Ridondanti} ovvero quelli dei quali l'informazione è già contenuta in altri attributi, la rilevanza va sempre valutata in base ai dati a disposizione
	\item \textit{Irrilevanti} contengono informazioni non utili per risolvere il problema di data mining.
\end{itemize}
\end{defn}

Vi sono diversi \textbf{approcci} per rilevare questi attributi:
\begin{itemize}
	\item \textit{Brute-force:} si provano tutti i modelli per ogni combinazione di parametri usati nel modello di Classificazione. La computazione di questi casi è troppo grossa, vi sono: \[\sum_{n=1}^{10}\binom{10}{n}\] possibili combinazioni
	\item \textit{Embedded:} gli attributi sono scelti in base alla capacità del modello della classificazione
	\item \textit{Filter}: gli attributi sono selezionati prima del classificatore in base a quelli che si ritengono più rilevanti e meno rilevanti (analisi di funzione obiettivo)
	\item \textit{Wrapper}: gli attributi si scelgono in base al modello di classificazione scelto, quelli che sono rilevanti per un modello non lo sono per un altro (dipende dall'ipotesi che ho fatto)
\end{itemize}
Politica Filter \& Wrapper a confronto
\begin{figure}[H]
	\centering
	\includegraphics[height=0.5 \linewidth]{classification/pict/filter_wrapper.png}
	\caption{Filter - Wrapper, due differenti approcci per feature selection}
\end{figure}
\textit{Non va bene fare feature selection su tutti i dati e poi trainare solo su un sottoinsieme!! altrimenti si hanno risultati non comparabili}

\subsubsection{Filter} 
Nel caso di attributi \textbf{Uni-variati}: 
\begin{enumerate}
	\item Si sceglie una \textit{misura di associazione} tra gli attributi candidato e quello di classe
	\item Si \textit{ordinano} gli attributi in base alla misura di associazione
	\item Si \textit{selezionano} le prime $R$ migliori posizioni come attributi di input per il classificatore
\end{enumerate}
Solitamente seguendo questa procedura si identificano correttamente gli \textit{attributi irrilevanti}; non è detto, però, che funzioni bene nella ricerca di \textit{attributi ridondanti}.
Nel caso di attributi \textbf{Multi-variati}:
\begin{itemize}
	\item Si identificano congiuntamente \textit{attributi rilevanti} e \textit{irrilevanti}
	\item Un buon sottoinsieme di attributi deve contenere attributi fortemente associati con l'attributo di classe ma essere incorrelati tra di loro
\end{itemize}
Le tecniche più utilizzate sono:
\begin{figure}[H]
	\centering
	\includegraphics[height=0.35 \linewidth]{classification/pict/feature_tecniques.png}
	\caption{Tecniche uni-multi variate per filtrare le features}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth] {classification/pict/filter_uni_multi.png}
	\caption{Vantaggi e svantaggi tra tecniche uni-variate e multi-variate}
\end{figure}
\noindent
I \textbf{vantaggi} per la feature selection sono:
\begin{itemize}
	\item Riduzione del costo di collezione di dati
	\item Riduzione dei tempi di inferenza del classificatore relativo all'attributo di classe
	\item Classificatore pi\`u interpretabile
	\item Aumento dell'accuratezza
\end{itemize}
Le \textbf{motivazioni} per cui questo approccio viene utilizzato sono:
\begin{itemize}
	\item Evitare l'overfitting
	\item Sviluppo di un miglior cost-effective classificatore
	\item Migliorare la comprensione del processo di generazione dati
\end{itemize}

Flowchart da seguire:
\begin{figure}[H]
	\centering
	\includegraphics[height=0.45 \linewidth]{classification/pict/feature_flowchart.png}
	\caption{flowchart per la feature selection}
\end{figure}

È spesso possibile creare, dagli attributi originali, un nuovo insieme di attributi che cattura informazioni rilevanti nel dataset in modo più efficace; inoltre, il numero di nuovi attributi può essere più piccolo rispetto al numero di attributi originali.\\
Le metodologie per la creazione di nuovi attributi (features) sono:
\begin{itemize}
	\item \textbf{Feature extraction}: una ampia gamma di modelli possono essere applicati in domini specifici come per classificazione di immagini/segnali
	\item \textbf{Mappare dati in un uovo spazio}: vedere gli attributi in modo normalizzato o secondo un'altra scala più facilmente trattabile (es. SVM) può migliorare di molto le performance
	\item \textbf{Feature construction}: generare feature nuove più comode per un certo modello di classificatore rispetto a quelle originali attraverso trasformazioni, es. logaritmi di somme ecc.. (non \`e detto che i dati originali siano i pi\`u utili per la classificazione)
\end{itemize}
\subsubsection{Regularization}
Alcuni modelli di Machine Learning ammettono iper-parametri. \`E dimostrato che una buona scelta dell'architettura di una rete neurale  permette  di approssimare qualsiasi problema. Non è però esente da overfitting, anzi, visto che non è sempre chiaro il funzionamento, è più difficile valutare quando sia presente. Si cerca di scegliere quella allocazione di parametri che riduce il più possibile l'errore quadratico del modello, quindi l'iperparametro $\lambda$ detto di regolarizzazione:

\[E(\textbf{w},\lambda) = \frac{1}{m} \sum_{i=1}^{m}(y_i - \hat{y}_i)^2 + \frac{\lambda}{2} \sum_{j=1}{K} w_i^2 \]

$K$: numero di parametri liberi (weights + thresholds) della rete neurale

$\lambda$: parametro di regolamentazione\\
La prima parte della formula riguarda il fitting della funzione, la seconda parte \`e la flessibilit\`a alla quale posso accedere.\\

\textit{Non si deve usare il training set per ottimizzare il parametro di regolarizzazione $\lambda$. Si andrebbe ad overfittare il modello. Se viene utilizzato il test set per ottimizzare il parametro lambda allora si è bruciato il dataset e devo rifare il modello, l'obiettivo è sempre quello di riconoscere dati mai visti, per questo si adottano schemi di divisione de dataset diversi.}

\subsubsection{Schema division}
Lo schema di divisione del dataset ottimale potrebbe essere questo:
\begin{figure}[H]
	\centering
	\includegraphics[height=0.3 \linewidth]{classification/pict/schema_dataset.png}
	\caption{Diversi schemi di divisione del dataset}
\end{figure}

\begin{itemize}
	\item Il train dataset viene utilizzato per trainare il modello (sui parametri w)
	\item Il parametro $\lambda$ viene ottimizzato usando il validation set
	\item Il test set viene usato per fornire una stima delle performance senza bias
	\item Può essere fatto tramite holdout, iterated holdout e cross-validation
\end{itemize}
Quando viene usato l'approccio \textbf{Filter} per la feature selection solitamente si usa uno schema \textbf{Train/Test}
\begin{itemize}
	\item Rilevanza e/o ridondanza sono stimate usando il Train set
	\item Dopo aver selezionato gli attributi il Train set viene usato per allenare il classificatore
	\item Le stime delle performance sono ottenute applicando il classificatore al Test set
\end{itemize}
Quando viene applicato l'approccio \textbf{Wrapper} per la feature selection solitamente si usa uno schema \textbf{Train/Validation/Test}
\begin{itemize}
	\item Il Validation set viene usato per ottimizzare le performance quando vengono usati diversi attributi per il modello (i wrapper)
	\item Selezionare sottoinsiemi ottimali di attributi è lo stesso che selezionare il valore ottimo del parametro regolarizzato $\lambda$
	\item Una volta selezionati gli attributi, il Train set e il Validation set vengono uniti e usati per addestrare il classificatore (tipicamente lo stesso tipo di classificatore usato per la feature selection)
	\item Le stime delle performance sono ottenute applicando il classificatore al Test set
\end{itemize}

\subsection{Classificazione NON binaria}
Nei problemi reali la classificazione avviene su più valori non solo binari.
\begin{defn}
	Un problema di classificazione \textbf{Non binaria} è un problema di classificazione che si divide in:
	\begin{itemize}
		\item \textbf{Multi-classe}: esattamente una classe si realizza
		\item \textbf{Multi-etichetta}: più di una classe può verificarsi
	\end{itemize}
\end{defn}
Vi possono essere anche problemi non di classe ma di \textbf{ranking} in cui i valori assunti dalla variabile di classe sono ordinati.
Per la risoluzione di questi problemi solitamente si adotta una logica \textbf{One-Vs-All}.

\subsubsection{One-Vs-All}
L'idea è quella di trasformare un problema multi-classe in tanti attributi di classe binari. 
In questa modalità si verifica se il set corrente verifica o meno ciascuna delle caratteristiche da valutare. 

\textit{Bisogna ricordare che non sempre binarizzare è necessario, in particolare per quanto riguarda i naive bayes non serve, invece per un decision tree sì.} 
\\Nel \textbf{One-vs-all} si crea un numero di \textit{classificatori binari} diverso in base al numero di modalità dell'attributo di classe; bisogna, inoltre, normalizzare poi gli output dei classificatori. 
Nel modello \textit{Multi-class} bisogna raccogliere i valori risultanti dai classificatori e fissare un threshold sopra il quale considero signficiativo il risultato (potenzialmente pi\`u di una classe supera il threshold).
\begin{figure}[H]
	\centering
	\includegraphics[height=0.4 \linewidth]{classification/pict/es_no_binary_classification.png}
	\caption{Esempio di classificatori binari}
\end{figure} 
